# -*- coding: utf-8 -*-
import time
import pandas as pd
import re
import sys
import io
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8ï¼Œé¿å…Windowsä¸‹emojiå­—ç¬¦ç¼–ç é”™è¯¯
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

class CNKIDataScraper:
    def __init__(self):
        self.driver = None
        self.papers_data = []

    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(10)
            return True
            
        except Exception as e:
            print(f"æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            return False

    def wait_for_results(self, timeout=30):
        """ç­‰å¾…æœç´¢ç»“æœåŠ è½½"""
        try:
            wait = WebDriverWait(self.driver, timeout)
            # ç­‰å¾…tbodyåŠ è½½
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "tbody")))
            # ç­‰å¾…è‡³å°‘æœ‰ä¸€ä¸ªclass="name"çš„å…ƒç´ ï¼ˆè®ºæ–‡åï¼‰
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "tbody .name")))
            return True
        except TimeoutException:
            print("ç­‰å¾…ç»“æœè¶…æ—¶")
            return False
        except Exception as e:
            print(f"ç­‰å¾…ç»“æœå¤±è´¥: {e}")
            return False

    def extract_papers_from_page(self):
        """ä»å½“å‰é¡µé¢æå–è®ºæ–‡ä¿¡æ¯"""
        papers = []
        
        try:
            # æŸ¥æ‰¾tbodyä¸­çš„æ‰€æœ‰è¡Œ
            tbody = self.driver.find_element(By.TAG_NAME, "tbody")
            rows = tbody.find_elements(By.TAG_NAME, "tr")
            
            for row in rows:
                paper = self.extract_paper_from_row(row)
                if paper and paper['title']:
                    papers.append(paper)
            
            return papers
            
        except Exception as e:
            print(f"æå–è®ºæ–‡ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def extract_paper_from_row(self, row):
        """ä»è¡¨æ ¼è¡Œä¸­æå–è®ºæ–‡ä¿¡æ¯"""
        paper = {
            'title': '',
            'authors': '',
            'journal': '',
            'publish_time': '',
            'link': ''
        }

        try:
            # æå–è®ºæ–‡åå’Œé“¾æ¥ï¼ˆclass="name"çš„aæ ‡ç­¾ï¼‰
            try:
                name_element = row.find_element(By.CSS_SELECTOR, ".name")
                # æŸ¥æ‰¾nameå…ƒç´ å†…çš„aæ ‡ç­¾
                link_element = name_element.find_element(By.TAG_NAME, "a")
                paper['title'] = link_element.text.strip()
                paper['link'] = link_element.get_attribute('href') or ''
            except NoSuchElementException:
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹å¼
                try:
                    name_element = row.find_element(By.CSS_SELECTOR, ".name")
                    paper['title'] = name_element.text.strip()
                    # å°è¯•åœ¨nameå…ƒç´ å†…æŸ¥æ‰¾é“¾æ¥
                    try:
                        link_element = name_element.find_element(By.TAG_NAME, "a")
                        paper['link'] = link_element.get_attribute('href') or ''
                    except:
                        pass
                except:
                    return None

            if not paper['title']:
                return None

            # æå–ä½œè€…ï¼ˆclass="author"ï¼‰
            try:
                author_element = row.find_element(By.CSS_SELECTOR, ".author")
                paper['authors'] = author_element.text.strip()
            except NoSuchElementException:
                paper['authors'] = ''

            # æå–æœŸåˆŠåï¼ˆclass="source"ï¼‰
            try:
                source_element = row.find_element(By.CSS_SELECTOR, ".source")
                paper['journal'] = source_element.text.strip()
            except NoSuchElementException:
                paper['journal'] = ''

            # æå–å‘è¡¨æ—¶é—´ï¼ˆclass="date"ï¼‰
            try:
                date_element = row.find_element(By.CSS_SELECTOR, ".date")
                date_text = date_element.text.strip()
                # æå–æ—¥æœŸæ ¼å¼
                date_match = re.search(r'(19|20)\d{2}-\d{1,2}-\d{1,2}', date_text)
                if date_match:
                    paper['publish_time'] = date_match.group()
                else:
                    # å°è¯•æå–å¹´ä»½
                    year_match = re.search(r'(19|20)\d{2}', date_text)
                    if year_match:
                        paper['publish_time'] = year_match.group()
                    else:
                        paper['publish_time'] = date_text
            except NoSuchElementException:
                paper['publish_time'] = ''

            # æ¸…ç†æ•°æ®
            for key in paper:
                if isinstance(paper[key], str):
                    paper[key] = re.sub(r'\s+', ' ', paper[key]).strip()

            return paper

        except Exception as e:
            # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›None
            return None

    def apply_date_filter(self, start_year, end_year):
        """åœ¨CNKIé¡µé¢ä¸Šåº”ç”¨æ—¥æœŸç­›é€‰"""
        try:
            time.sleep(2)
            
            # å°è¯•é€šè¿‡å·¦ä¾§ç­›é€‰æ è®¾ç½®å‘è¡¨æ—¶é—´
            try:
                time_filter_xpaths = [
                    "//div[contains(@class, 'filter')]//a[contains(text(), 'å‘è¡¨æ—¶é—´')]",
                    "//div[contains(@class, 'filter')]//a[contains(text(), 'æ—¶é—´èŒƒå›´')]",
                    "//span[contains(text(), 'å‘è¡¨æ—¶é—´')]",
                    "//label[contains(text(), 'å‘è¡¨æ—¶é—´')]"
                ]
                
                for xpath in time_filter_xpaths:
                    try:
                        time_filter = self.driver.find_element(By.XPATH, xpath)
                        if time_filter.is_displayed():
                            self.driver.execute_script("arguments[0].click();", time_filter)
                            time.sleep(1)
                            break
                    except:
                        continue
            except:
                pass
            
            # å°è¯•ç›´æ¥æ“ä½œå¹´ä»½è¾“å…¥æ¡†
            try:
                start_inputs = [
                    "input[name*='start']",
                    "input[name*='begin']",
                    "input[id*='start']",
                    "input[id*='YearFrom']"
                ]
                
                for selector in start_inputs:
                    try:
                        start_input = self.driver.find_element(By.CSS_SELECTOR, selector)
                        if start_input.is_displayed():
                            start_input.clear()
                            start_input.send_keys(str(start_year))
                            time.sleep(0.5)
                            break
                    except:
                        continue
                
                end_inputs = [
                    "input[name*='end']",
                    "input[name*='finish']",
                    "input[id*='end']",
                    "input[id*='YearTo']"
                ]
                
                for selector in end_inputs:
                    try:
                        end_input = self.driver.find_element(By.CSS_SELECTOR, selector)
                        if end_input.is_displayed():
                            end_input.clear()
                            end_input.send_keys(str(end_year))
                            time.sleep(0.5)
                            break
                    except:
                        continue
                
                # å°è¯•ç‚¹å‡»"ç¡®å®š"æˆ–"æ£€ç´¢"æŒ‰é’®
                confirm_xpaths = [
                    "//button[contains(text(), 'ç¡®å®š')]",
                    "//button[contains(text(), 'æ£€ç´¢')]",
                    "//a[contains(text(), 'æ£€ç´¢')]"
                ]
                
                for xpath in confirm_xpaths:
                    try:
                        confirm_btn = self.driver.find_element(By.XPATH, xpath)
                        if confirm_btn.is_displayed():
                            self.driver.execute_script("arguments[0].click();", confirm_btn)
                            time.sleep(3)
                            break
                    except:
                        continue
                        
            except Exception as e:
                print(f"è®¾ç½®æ—¥æœŸç­›é€‰æ—¶å‡ºé”™: {e}")
            
            return True
            
        except Exception as e:
            print(f"åº”ç”¨æ—¥æœŸç­›é€‰å¤±è´¥: {e}")
            return False

    def filter_papers_by_year(self, papers, target_year):
        """è¿‡æ»¤è®ºæ–‡ï¼Œåªä¿ç•™æŒ‡å®šå¹´ä»½çš„è®ºæ–‡"""
        filtered = []
        for paper in papers:
            publish_time = paper.get('publish_time', '')
            if not publish_time:
                continue
            
            year_match = re.search(r'(19|20)\d{2}', str(publish_time))
            if year_match:
                year = int(year_match.group())
                if year == target_year:
                    filtered.append(paper)
        
        return filtered

    def scrape_papers(self, url, max_pages=10, target_year=None):
        """æŠ“å–è®ºæ–‡æ•°æ®"""
        if not self.setup_driver():
            return []
        
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®é“¾æ¥: {url}")
            self.driver.get(url)
            time.sleep(5)
            
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å¹´ä»½ï¼Œå°è¯•åœ¨é¡µé¢ä¸Šè®¾ç½®æ—¶é—´ç­›é€‰
            if target_year:
                print(f"ğŸ“… æ­£åœ¨è®¾ç½®æ—¶é—´ç­›é€‰ï¼š{target_year}å¹´")
                self.apply_date_filter(target_year, target_year)
                time.sleep(3)
            
            all_papers = []
            
            for page in range(1, max_pages + 1):
                print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ...")
                
                if page > 1:
                    try:
                        # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                        next_button = None
                        
                        # æ–¹æ³•1: æŸ¥æ‰¾class="pagesnums"ä¸”æ–‡æœ¬ä¸º"ä¸‹ä¸€é¡µ"
                        try:
                            next_button = self.driver.find_element(By.XPATH, "//a[@class='pagesnums' and text()='ä¸‹ä¸€é¡µ']")
                        except:
                            pass
                        
                        # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«"ä¸‹ä¸€"æ–‡æœ¬çš„é“¾æ¥
                        if not next_button:
                            try:
                                next_button = self.driver.find_element(By.XPATH, "//a[@class='pagesnums' and contains(text(), 'ä¸‹ä¸€')]")
                            except:
                                pass
                        
                        # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰pagesnumsé“¾æ¥
                        if not next_button:
                            try:
                                pagesnums_links = self.driver.find_elements(By.CSS_SELECTOR, "a.pagesnums")
                                for link in pagesnums_links:
                                    if link.text.strip() == "ä¸‹ä¸€é¡µ" or "ä¸‹ä¸€" in link.text:
                                        next_button = link
                                        break
                            except:
                                pass
                        
                        if next_button:
                            if next_button.is_displayed():
                                self.driver.execute_script("arguments[0].click();", next_button)
                                print(f"ğŸ“„ ç¿»åˆ°ç¬¬ {page} é¡µ...")
                                time.sleep(3)
                            else:
                                print(f"âš ï¸ ä¸‹ä¸€é¡µæŒ‰é’®ä¸å¯è§ï¼Œåœæ­¢ç¿»é¡µ")
                                break
                        else:
                            print(f"âš ï¸ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ")
                            break
                            
                    except Exception as e:
                        print(f"âš ï¸ ç¿»é¡µå¤±è´¥: {e}")
                        break
                
                if self.wait_for_results():
                    papers = self.extract_papers_from_page()
                    if papers:
                        all_papers.extend(papers)
                        print(f"âœ… ç¬¬ {page} é¡µæå–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                    else:
                        if page > 1:
                            break
                else:
                    break
                
                if page < max_pages:
                    time.sleep(2)
            
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å¹´ä»½ï¼Œå†æ¬¡è¿‡æ»¤æ•°æ®ç¡®ä¿å‡†ç¡®æ€§
            if target_year:
                print(f"ğŸ” æ­£åœ¨è¿‡æ»¤æ•°æ®ï¼Œåªä¿ç•™{target_year}å¹´çš„è®ºæ–‡...")
                all_papers = self.filter_papers_by_year(all_papers, target_year)
                print(f"âœ… è¿‡æ»¤åå‰©ä½™ {len(all_papers)} ç¯‡è®ºæ–‡")
            
            self.papers_data = all_papers
            return all_papers
            
        except Exception as e:
            print(f"æŠ“å–è¿‡ç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
        
        finally:
            if self.driver:
                self.driver.quit()

    def save_to_excel(self, papers, filename=None):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"çŸ¥ç½‘æ•°æ®_{timestamp}.xlsx"
        
        if not papers:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            data = []
            for paper in papers:
                data.append({
                    'ä½œè€…': paper.get('authors', ''),
                    'è®ºæ–‡å': paper.get('title', ''),
                    'æœŸåˆŠå': paper.get('journal', ''),
                    'å‘è¡¨æ—¶é—´': paper.get('publish_time', ''),
                    'é“¾æ¥': paper.get('link', '')
                })
            
            df = pd.DataFrame(data)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
            
        except Exception as e:
            print(f"ä¿å­˜å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¿è¡ŒçŸ¥ç½‘æ•°æ®æŠ“å–å·¥å…·...")
    
    # çŸ¥ç½‘é“¾æ¥
    url = "https://kns.cnki.net/kns8s/defaultresult/index?crossids=YSTT4HG0%2CLSTPFY1C%2CJUP3MUPD%2CMPMFIG1A%2CWQ0UVIAA%2CBLZOG7CK%2CPWFIRAGL%2CEMRPGLPA%2CNLBO1Z6R%2CNN3FJMUV&korder=AF&kw=%E9%A6%96%E9%83%BD%E5%9B%BE%E4%B9%A6%E9%A6%86"
    
    # è·å–å»å¹´çš„å¹´ä»½
    current_year = datetime.now().year
    last_year = current_year - 1
    print(f"ğŸ“… ç›®æ ‡æ—¶é—´èŒƒå›´ï¼š{last_year}å¹´ï¼ˆå»å¹´ï¼‰")
    
    try:
        scraper = CNKIDataScraper()
        print(f"ğŸ” å¼€å§‹æŠ“å–{last_year}å¹´çš„è®ºæ–‡æ•°æ®...")
        papers = scraper.scrape_papers(url, max_pages=10, target_year=last_year)
        
        if papers:
            print(f"âœ… æˆåŠŸæŠ“å–åˆ° {len(papers)} ç¯‡{last_year}å¹´çš„è®ºæ–‡")
            print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°Excelæ–‡ä»¶...")
            scraper.save_to_excel(papers, filename=f"çŸ¥ç½‘æ•°æ®_{last_year}å¹´.xlsx")
            print("ğŸ‰ æŠ“å–å®Œæˆï¼")
        else:
            print(f"âŒ æ²¡æœ‰æŠ“å–åˆ°{last_year}å¹´çš„æ•°æ®")
    
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

