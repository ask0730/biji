import time
import pandas as pd
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class WanfangDataScraper:
    def __init__(self):
        self.driver = None
        self.papers_data = []

    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.implicitly_wait(10)
            return True
            
        except Exception as e:
            print(f"æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            return False

    def wait_for_results(self, timeout=30):
        """ç­‰å¾…æœç´¢ç»“æœåŠ è½½"""
        try:
            wait = WebDriverWait(self.driver, timeout)
            # ç­‰å¾…ä¸‡æ–¹æ•°æ®çš„å…³é”®å…ƒç´ åŠ è½½
            selectors = [
                ".title-area",
                ".right-content",
                ".result-item",
                "[class*='result']",
                "[class*='paper']"
            ]
            
            for selector in selectors:
                try:
                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                    print(f"é¡µé¢å…ƒç´ åŠ è½½å®Œæˆ: {selector}")
                    break
                except TimeoutException:
                    continue
            
            time.sleep(3)  # é¢å¤–ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½
            return True
            
        except Exception as e:
            print(f"ç­‰å¾…ç»“æœå¤±è´¥: {e}")
            return False

    def extract_paper_from_element(self, element):
        """ä»åˆ—è¡¨é¡µå…ƒç´ ä¸­ç‚¹å‡»æ ‡é¢˜è¿›å…¥è¯¦æƒ…é¡µå¹¶æå–è®ºæ–‡ä¿¡æ¯"""
        list_page_handle = None
        list_page_url = None
        try:
            # ä¿å­˜å½“å‰çª—å£å¥æŸ„å’ŒURLï¼Œç”¨äºè¿”å›åˆ—è¡¨é¡µ
            list_page_handle = self.driver.current_window_handle
            list_page_url = self.driver.current_url
            
            # æŸ¥æ‰¾å¹¶ç‚¹å‡» <span class="title"> å…ƒç´ 
            try:
                title_element = element.find_element(By.CSS_SELECTOR, "span.title")
                
                # æ»šåŠ¨åˆ°å…ƒç´ å¯è§
                self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", title_element)
                time.sleep(0.5)
                
                # è·å–ç‚¹å‡»å‰çš„çª—å£æ•°é‡
                window_handles_before = self.driver.window_handles
                print(f"  ç‚¹å‡»å‰çª—å£æ•°é‡: {len(window_handles_before)}")
                
                try:
                    # å°è¯•ç›´æ¥ç‚¹å‡»
                    title_element.click()
                except:
                    # å¦‚æœç›´æ¥ç‚¹å‡»å¤±è´¥ï¼Œä½¿ç”¨JavaScriptç‚¹å‡»
                    self.driver.execute_script("arguments[0].click();", title_element)
                
                # ç­‰å¾…æ–°æ ‡ç­¾é¡µå‡ºç°
                print("  ç­‰å¾…æ–°æ ‡ç­¾é¡µæ‰“å¼€...")
                new_window_handle = None
                try:
                    wait = WebDriverWait(self.driver, 10)
                    # ç­‰å¾…çª—å£æ•°é‡å¢åŠ 
                    wait.until(lambda driver: len(driver.window_handles) > len(window_handles_before))
                    
                    # è·å–æ‰€æœ‰çª—å£å¥æŸ„ï¼Œæ‰¾åˆ°æ–°æ‰“å¼€çš„çª—å£
                    all_handles = self.driver.window_handles
                    for handle in all_handles:
                        if handle not in window_handles_before:
                            new_window_handle = handle
                            break
                    
                    if new_window_handle:
                        print(f"  âœ… æ–°æ ‡ç­¾é¡µå·²æ‰“å¼€")
                        # åˆ‡æ¢åˆ°æ–°æ ‡ç­¾é¡µ
                        self.driver.switch_to.window(new_window_handle)
                        current_url = self.driver.current_url
                        print(f"  æ–°æ ‡ç­¾é¡µURL: {current_url[:100]}...")
                    else:
                        print("  âš ï¸ æœªæ‰¾åˆ°æ–°æ ‡ç­¾é¡µå¥æŸ„")
                        return None
                        
                except TimeoutException:
                    print("  âš ï¸ ç­‰å¾…æ–°æ ‡ç­¾é¡µè¶…æ—¶ï¼Œå¯èƒ½æœªæ‰“å¼€æ–°æ ‡ç­¾é¡µ")
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çª—å£
                    all_handles = self.driver.window_handles
                    if len(all_handles) > len(window_handles_before):
                        for handle in all_handles:
                            if handle not in window_handles_before:
                                new_window_handle = handle
                                self.driver.switch_to.window(handle)
                                print(f"  âœ… æ‰¾åˆ°æ–°æ ‡ç­¾é¡µï¼Œå·²åˆ‡æ¢")
                                break
                    else:
                        print("  âš ï¸ æœªæ£€æµ‹åˆ°æ–°æ ‡ç­¾é¡µï¼Œç»§ç»­å°è¯•...")
                        return None
                
                # ç­‰å¾…è¯¦æƒ…é¡µçš„å…³é”®å…ƒç´ åŠ è½½
                element_found = False
                try:
                    wait = WebDriverWait(self.driver, 10)
                    wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "div.detailTitleCN")))
                    print("  è¯¦æƒ…é¡µå…³é”®å…ƒç´ å·²å¯è§")
                    element_found = True
                except TimeoutException:
                    # å¦‚æœç­‰å¾…å¯è§è¶…æ—¶ï¼Œå°è¯•æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨ï¼ˆå¯èƒ½å·²å­˜åœ¨ä½†ä¸å¯è§ï¼‰
                    try:
                        element = self.driver.find_element(By.CSS_SELECTOR, "div.detailTitleCN")
                        if element:
                            print("  è¯¦æƒ…é¡µå…ƒç´ å·²å­˜åœ¨ï¼ˆå¯èƒ½ä¸å¯è§ï¼‰ï¼Œç»§ç»­æå–...")
                            element_found = True
                    except NoSuchElementException:
                        # å†ç­‰å¾…ä¸€ä¸‹ï¼Œå¯èƒ½è¿˜åœ¨åŠ è½½
                        time.sleep(2)
                        try:
                            element = self.driver.find_element(By.CSS_SELECTOR, "div.detailTitleCN")
                            if element:
                                print("  è¯¦æƒ…é¡µå…ƒç´ å·²æ‰¾åˆ°ï¼Œç»§ç»­æå–...")
                                element_found = True
                        except:
                            print("  è­¦å‘Šï¼šæœªæ‰¾åˆ°è¯¦æƒ…é¡µå…ƒç´ ï¼Œä½†ç»§ç»­å°è¯•æå–...")
                
                if not element_found:
                    # é¢å¤–ç­‰å¾…ä¸€ä¸‹ï¼Œç»™é¡µé¢æ›´å¤šåŠ è½½æ—¶é—´
                    time.sleep(2)
                
                # ä»è¯¦æƒ…é¡µæå–ä¿¡æ¯
                paper = self.extract_paper_from_detail_page()
                
                # å…³é—­æ–°æ ‡ç­¾é¡µï¼Œåˆ‡æ¢å›åˆ—è¡¨é¡µ
                if new_window_handle:
                    self.driver.close()  # å…³é—­æ–°æ ‡ç­¾é¡µ
                    time.sleep(0.5)
                
                # åˆ‡æ¢å›åˆ—è¡¨é¡µæ ‡ç­¾é¡µ
                if list_page_handle and list_page_handle in self.driver.window_handles:
                    self.driver.switch_to.window(list_page_handle)
                    print("  âœ… å·²åˆ‡æ¢å›åˆ—è¡¨é¡µ")
                    time.sleep(1)
                else:
                    # å¦‚æœåŸæ ‡ç­¾é¡µå·²å…³é—­ï¼Œå°è¯•åˆ‡æ¢åˆ°ç¬¬ä¸€ä¸ªçª—å£
                    if self.driver.window_handles:
                        self.driver.switch_to.window(self.driver.window_handles[0])
                        print("  âš ï¸ åŸæ ‡ç­¾é¡µå¯èƒ½å·²å…³é—­ï¼Œå·²åˆ‡æ¢åˆ°ç¬¬ä¸€ä¸ªçª—å£")
                
                return paper
                
            except NoSuchElementException:
                print("  æœªæ‰¾åˆ°æ ‡é¢˜å…ƒç´ ")
                return None
                
        except Exception as e:
            print(f"  ç‚¹å‡»æ ‡é¢˜è¿›å…¥è¯¦æƒ…é¡µæ—¶å‡ºé”™: {e}")
            # å°è¯•è¿”å›åˆ—è¡¨é¡µ
            try:
                if list_page_url:
                    self.driver.get(list_page_url)
                    time.sleep(2)
            except:
                pass
            return None

    def extract_papers_from_page(self):
        """ä»å½“å‰é¡µé¢æå–è®ºæ–‡ä¿¡æ¯"""
        papers = []
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡é¡¹
            # ä¸‡æ–¹æ•°æ®å¯èƒ½ä½¿ç”¨å¤šç§ç»“æ„ï¼Œå°è¯•ä¸åŒçš„é€‰æ‹©å™¨
            paper_elements = []
            
            # æ–¹æ³•1: æŸ¥æ‰¾ .title-area
            try:
                title_areas = self.driver.find_elements(By.CSS_SELECTOR, ".title-area")
                if title_areas:
                    paper_elements = title_areas
                    print(f"æ‰¾åˆ° {len(paper_elements)} ä¸ª .title-area å…ƒç´ ")
            except:
                pass
            
            # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
            if not paper_elements:
                try:
                    result_items = self.driver.find_elements(By.CSS_SELECTOR, ".result-item, [class*='result-item'], [class*='paper-item']")
                    if result_items:
                        paper_elements = result_items
                        print(f"æ‰¾åˆ° {len(paper_elements)} ä¸ªç»“æœé¡¹")
                except:
                    pass
            
            # æ–¹æ³•3: æŸ¥æ‰¾åŒ…å« span.title çš„å®¹å™¨
            if not paper_elements:
                try:
                    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« span.title çš„å®¹å™¨
                    containers = self.driver.find_elements(By.CSS_SELECTOR, "div[class*='item'], div[class*='result'], div[class*='paper'], .title-area")
                    paper_elements = [c for c in containers if c.find_elements(By.CSS_SELECTOR, "span.title")]
                    if paper_elements:
                        print(f"æ‰¾åˆ° {len(paper_elements)} ä¸ªåŒ…å« span.title çš„è®ºæ–‡å®¹å™¨")
                except:
                    pass
            
            # ä¿å­˜åˆ—è¡¨é¡µURL
            list_page_url = self.driver.current_url
            
            # æå–æ¯ç¯‡è®ºæ–‡
            total_count = len(paper_elements)
            for i in range(total_count):
                try:
                    print(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{total_count} ç¯‡è®ºæ–‡...")
                    
                    # æ¯æ¬¡å¾ªç¯å‰é‡æ–°è·å–å…ƒç´ ï¼ˆå› ä¸ºé¡µé¢å¯èƒ½è¢«é‡æ–°åŠ è½½ï¼‰
                    try:
                        # ä¼˜å…ˆæŸ¥æ‰¾åŒ…å« span.title çš„å…ƒç´ å®¹å™¨
                        containers = self.driver.find_elements(By.CSS_SELECTOR, ".title-area, .result-item, [class*='result-item'], [class*='paper-item']")
                        current_elements = [c for c in containers if c.find_elements(By.CSS_SELECTOR, "span.title")]
                        
                        if not current_elements:
                            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾ .title-area
                            current_elements = self.driver.find_elements(By.CSS_SELECTOR, ".title-area")
                        
                        if i < len(current_elements):
                            element = current_elements[i]
                        else:
                            print(f"  ç¬¬ {i+1} ä¸ªå…ƒç´ ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                            continue
                    except Exception as e:
                        print(f"  é‡æ–°è·å–å…ƒç´ å¤±è´¥: {e}")
                        # å°è¯•é‡æ–°åŠ è½½åˆ—è¡¨é¡µ
                        try:
                            self.driver.get(list_page_url)
                            time.sleep(3)
                            containers = self.driver.find_elements(By.CSS_SELECTOR, ".title-area, .result-item")
                            current_elements = [c for c in containers if c.find_elements(By.CSS_SELECTOR, "span.title")]
                            if not current_elements:
                                current_elements = self.driver.find_elements(By.CSS_SELECTOR, ".title-area")
                            if i < len(current_elements):
                                element = current_elements[i]
                            else:
                                continue
                        except:
                            continue
                    
                    # æå–è®ºæ–‡ä¿¡æ¯
                    paper = self.extract_paper_from_element(element)
                    if paper and paper['title']:
                        papers.append(paper)
                        print(f"âœ… ç¬¬ {i+1} ç¯‡è®ºæ–‡æå–æˆåŠŸ: {paper['title'][:50]}...")
                    else:
                        print(f"âš ï¸ ç¬¬ {i+1} ç¯‡è®ºæ–‡æå–å¤±è´¥")
                        
                except Exception as e:
                    print(f"æå–ç¬¬ {i+1} ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {e}")
                    # ç¡®ä¿è¿”å›åˆ—è¡¨é¡µ
                    try:
                        if self.driver.current_url != list_page_url:
                            self.driver.get(list_page_url)
                            time.sleep(2)
                    except:
                        pass
                    continue
            
            return papers
            
        except Exception as e:
            print(f"æå–è®ºæ–‡ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def extract_paper_from_detail_page(self):
        """ä»è¯¦æƒ…é¡µæå–è®ºæ–‡ä¿¡æ¯"""
        paper = {
            'title': '',
            'authors': '',
            'journal': '',
            'publish_time': '',
            'link': ''
        }

        try:
            # æå–è®ºæ–‡åï¼ˆdiv.detailTitleCN ä¸‹çš„ span é‡Œé¢çš„æ–‡å­—ï¼‰
            try:
                title_div = self.driver.find_element(By.CSS_SELECTOR, "div.detailTitleCN")
                # æŸ¥æ‰¾ div ä¸‹çš„ span æ ‡ç­¾
                title_span = title_div.find_element(By.TAG_NAME, "span")
                paper['title'] = title_span.text.strip()
            except NoSuchElementException:
                # å¦‚æœæ‰¾ä¸åˆ° spanï¼Œå°è¯•ç›´æ¥è·å– div çš„æ–‡æœ¬
                try:
                    title_div = self.driver.find_element(By.CSS_SELECTOR, "div.detailTitleCN")
                    paper['title'] = title_div.text.strip()
                except:
                    print("  æœªæ‰¾åˆ°è®ºæ–‡å")
                    return None

            # æå–é“¾æ¥ï¼ˆå½“å‰URLï¼‰
            paper['link'] = self.driver.current_url

            # æå–ä½œè€…ï¼ˆdiv.author.detailTitle ä¸‹çš„ a æ ‡ç­¾çš„ span é‡Œï¼Œæ’é™¤ class="sup" çš„å…ƒç´ ï¼‰
            try:
                author_div = self.driver.find_element(By.CSS_SELECTOR, "div.author.detailTitle")
                author_links = author_div.find_elements(By.TAG_NAME, "a")
                
                authors_list = []
                for author_link in author_links:
                    # æ’é™¤ class="sup" çš„ a æ ‡ç­¾
                    link_class = author_link.get_attribute("class") or ""
                    if "sup" in link_class:
                        continue
                    
                    try:
                        # æŸ¥æ‰¾ a æ ‡ç­¾å†…çš„ spanï¼ˆæ’é™¤ class="sup" çš„ spanï¼‰
                        author_spans = author_link.find_elements(By.TAG_NAME, "span")
                        author_name = ""
                        for span in author_spans:
                            # æ’é™¤ class="sup" çš„ span
                            span_class = span.get_attribute("class") or ""
                            if "sup" in span_class:
                                continue
                            span_text = span.text.strip()
                            if span_text:
                                author_name = span_text
                                break
                        
                        # å¦‚æœæ‰¾åˆ°äº†æœ‰æ•ˆçš„ä½œè€…åï¼Œæ·»åŠ åˆ°åˆ—è¡¨
                        if author_name:
                            authors_list.append(author_name)
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„spanï¼Œä½¿ç”¨JavaScriptè·å–æ–‡æœ¬ï¼ˆæ’é™¤supå…ƒç´ ï¼‰
                            # å…ˆç§»é™¤supå…ƒç´ ï¼Œå†è·å–æ–‡æœ¬
                            try:
                                # ä½¿ç”¨JavaScriptè·å–æ–‡æœ¬ï¼Œæ’é™¤supå…ƒç´ 
                                author_name = self.driver.execute_script("""
                                    var link = arguments[0];
                                    var clone = link.cloneNode(true);
                                    var supElements = clone.querySelectorAll('.sup, [class*="sup"]');
                                    supElements.forEach(function(el) { el.remove(); });
                                    return clone.textContent.trim();
                                """, author_link)
                                if author_name:
                                    authors_list.append(author_name)
                            except:
                                # å¦‚æœJavaScriptå¤±è´¥ï¼Œç›´æ¥å–aæ ‡ç­¾çš„æ–‡æœ¬
                                author_name = author_link.text.strip()
                                if author_name:
                                    authors_list.append(author_name)
                    except NoSuchElementException:
                        # å¦‚æœæ²¡æœ‰spanï¼Œä½¿ç”¨JavaScriptè·å–æ–‡æœ¬ï¼ˆæ’é™¤supå…ƒç´ ï¼‰
                        try:
                            author_name = self.driver.execute_script("""
                                var link = arguments[0];
                                var clone = link.cloneNode(true);
                                var supElements = clone.querySelectorAll('.sup, [class*="sup"]');
                                supElements.forEach(function(el) { el.remove(); });
                                return clone.textContent.trim();
                            """, author_link)
                            if author_name:
                                authors_list.append(author_name)
                        except:
                            # å¦‚æœJavaScriptå¤±è´¥ï¼Œç›´æ¥å–aæ ‡ç­¾çš„æ–‡æœ¬
                            author_name = author_link.text.strip()
                            if author_name:
                                authors_list.append(author_name)
                
                paper['authors'] = ';'.join(authors_list)
            except NoSuchElementException:
                print("  æœªæ‰¾åˆ°ä½œè€…ä¿¡æ¯")
                paper['authors'] = ''

            # æå–æœŸåˆŠåï¼ˆclass="periodicalName" çš„ a æ ‡ç­¾é‡Œï¼‰
            try:
                # ä¼˜å…ˆå°è¯• a.periodicalNameï¼ˆaæ ‡ç­¾æœ‰periodicalNameç±»ï¼‰
                journal_element = self.driver.find_element(By.CSS_SELECTOR, "a.periodicalName")
                paper['journal'] = journal_element.text.strip()
            except NoSuchElementException:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯• .periodicalName aï¼ˆperiodicalNameç±»ä¸‹çš„aæ ‡ç­¾ï¼‰
                try:
                    journal_element = self.driver.find_element(By.CSS_SELECTOR, ".periodicalName a")
                    paper['journal'] = journal_element.text.strip()
                except:
                    print("  æœªæ‰¾åˆ°æœŸåˆŠå")
                    paper['journal'] = ''

            # æå–å‘è¡¨æ—¶é—´ï¼ˆclass="publish list" ä¸‹çš„ class="itemUrl"ï¼‰
            try:
                publish_list = self.driver.find_element(By.CSS_SELECTOR, ".publish.list")
                item_url = publish_list.find_element(By.CSS_SELECTOR, ".itemUrl")
                paper['publish_time'] = item_url.text.strip()
            except NoSuchElementException:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                try:
                    publish_list = self.driver.find_element(By.CSS_SELECTOR, "[class*='publish'][class*='list']")
                    item_url = publish_list.find_element(By.CSS_SELECTOR, "[class*='itemUrl']")
                    paper['publish_time'] = item_url.text.strip()
                except:
                    print("  æœªæ‰¾åˆ°å‘è¡¨æ—¶é—´")
                    paper['publish_time'] = ''

            # æ¸…ç†æ•°æ®
            for key in paper:
                if isinstance(paper[key], str):
                    paper[key] = re.sub(r'\s+', ' ', paper[key]).strip()

            return paper

        except Exception as e:
            print(f"  æå–è¯¦æƒ…é¡µä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None

    def filter_papers_by_year(self, papers, target_year):
        """è¿‡æ»¤è®ºæ–‡ï¼Œåªä¿ç•™æŒ‡å®šå¹´ä»½çš„è®ºæ–‡"""
        filtered = []
        for paper in papers:
            publish_time = paper.get('publish_time', '')
            if not publish_time:
                continue
            
            year_match = re.search(r'(19|20)\d{2}', str(publish_time))
            if year_match:
                year = int(year_match.group())
                if year == target_year:
                    filtered.append(paper)
        
        return filtered

    def scrape_papers(self, url, max_pages=1, target_year=None):
        """æŠ“å–è®ºæ–‡æ•°æ®"""
        if not self.setup_driver():
            return []
        
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®é“¾æ¥: {url}")
            self.driver.get(url)
            time.sleep(5)
            
            print(f"ğŸ“„ æ­£åœ¨æŠ“å–æ•°æ®...")
            
            all_papers = []
            if self.wait_for_results():
                papers = self.extract_papers_from_page()
                if papers:
                    all_papers = papers
                    print(f"âœ… æå–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            
            # ä¸å†è¿‡æ»¤æ—¶é—´ï¼Œæ‰€æœ‰è®ºæ–‡éƒ½ç¬¦åˆè¦æ±‚
            self.papers_data = all_papers
            return all_papers
            
        except Exception as e:
            print(f"æŠ“å–è¿‡ç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
        
        finally:
            if self.driver:
                self.driver.quit()

    def save_to_excel(self, papers, filename=None):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ä¸‡æ–¹æ•°æ®_{timestamp}.xlsx"
        
        if not papers:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            data = []
            for paper in papers:
                data.append({
                    'ä½œè€…': paper.get('authors', ''),
                    'è®ºæ–‡å': paper.get('title', ''),
                    'æœŸåˆŠå': paper.get('journal', ''),
                    'å‘è¡¨æ—¶é—´': paper.get('publish_time', ''),
                    'é“¾æ¥': paper.get('link', '')
                })
            
            df = pd.DataFrame(data)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
            
        except Exception as e:
            print(f"ä¿å­˜å¤±è´¥: {e}")
            return False


def build_url_with_year(base_url, year):
    """æ„å»ºåŒ…å«æŒ‡å®šå¹´ä»½çš„URL"""
    # å°†URLä¸­çš„Date:2024-2024æˆ–Date%3A2024-2024æ›¿æ¢ä¸ºæŒ‡å®šå¹´ä»½
    # æ³¨æ„ï¼šURLä¸­å¯èƒ½ä½¿ç”¨URLç¼–ç ï¼Œ%3Aæ˜¯å†’å·çš„ç¼–ç 
    new_url = base_url
    
    # å¤„ç†URLç¼–ç çš„æƒ…å†µï¼šDate%3A2024-2024
    if 'Date%3A' in new_url:
        new_url = re.sub(r'Date%3A\d{4}-\d{4}', f'Date%3A{year}-{year}', new_url)
    # å¤„ç†æœªç¼–ç çš„æƒ…å†µï¼šDate:2024-2024
    elif 'Date:' in new_url:
        new_url = re.sub(r'Date:\d{4}-\d{4}', f'Date:{year}-{year}', new_url)
    else:
        # å¦‚æœæ²¡æœ‰Dateå‚æ•°ï¼Œæ·»åŠ å®ƒï¼ˆä½¿ç”¨URLç¼–ç æ ¼å¼ï¼‰
        if '?' in new_url:
            new_url = f"{new_url}&Date%3A{year}-{year}"
        else:
            new_url = f"{new_url}?Date%3A{year}-{year}"
    
    return new_url


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¿è¡Œä¸‡æ–¹æ•°æ®æŠ“å–å·¥å…·...")
    
    # ä¸‡æ–¹æ•°æ®é“¾æ¥
    base_url = "https://s.wanfangdata.com.cn/paper?q=%28%E4%BD%9C%E8%80%85%E5%8D%95%E4%BD%8D%3A%E9%A6%96%E9%83%BD%E5%9B%BE%E4%B9%A6%E9%A6%86%29%20Date%3A2022-2022&p=1&s=100"
    
    # è·å–å»å¹´çš„å¹´ä»½
    current_year = datetime.now().year
    last_year = current_year - 1
    print(f"ğŸ“… ç›®æ ‡æ—¶é—´èŒƒå›´ï¼š{last_year}å¹´ï¼ˆå»å¹´ï¼‰")
    
    # æ„å»ºåŒ…å«å»å¹´å¹´ä»½çš„URL
    url = build_url_with_year(base_url, last_year)
    print(f"ğŸ”— ç›®æ ‡URL: {url}")
    
    try:
        scraper = WanfangDataScraper()
        print(f"ğŸ” å¼€å§‹æŠ“å–è®ºæ–‡æ•°æ®...")
        papers = scraper.scrape_papers(url, max_pages=10, target_year=None)
        
        if papers:
            print(f"âœ… æˆåŠŸæŠ“å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°Excelæ–‡ä»¶...")
            scraper.save_to_excel(papers, filename=f"ä¸‡æ–¹æ•°æ®_{last_year}å¹´.xlsx")
            print("ğŸ‰ æŠ“å–å®Œæˆï¼")
        else:
            print(f"âŒ æ²¡æœ‰æŠ“å–åˆ°æ•°æ®")
    
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

