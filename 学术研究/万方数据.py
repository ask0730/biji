import time
import pandas as pd
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class WanfangDataScraper:
    def __init__(self):
        self.driver = None
        self.papers_data = []

    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.implicitly_wait(10)
            return True
            
        except Exception as e:
            print(f"æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            return False

    def wait_for_results(self, timeout=30):
        """ç­‰å¾…æœç´¢ç»“æœåŠ è½½"""
        try:
            wait = WebDriverWait(self.driver, timeout)
            # ç­‰å¾…ä¸‡æ–¹æ•°æ®çš„å…³é”®å…ƒç´ åŠ è½½
            selectors = [
                ".title-area",
                ".right-content",
                ".result-item",
                "[class*='result']",
                "[class*='paper']"
            ]
            
            for selector in selectors:
                try:
                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                    print(f"é¡µé¢å…ƒç´ åŠ è½½å®Œæˆ: {selector}")
                    break
                except TimeoutException:
                    continue
            
            time.sleep(3)  # é¢å¤–ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½
            return True
            
        except Exception as e:
            print(f"ç­‰å¾…ç»“æœå¤±è´¥: {e}")
            return False

    def extract_paper_from_element(self, element):
        """ä»åˆ—è¡¨é¡µå…ƒç´ ä¸­ç‚¹å‡»æ ‡é¢˜è¿›å…¥è¯¦æƒ…é¡µå¹¶æå–è®ºæ–‡ä¿¡æ¯"""
        list_page_url = None
        try:
            # ä¿å­˜å½“å‰URLï¼Œç”¨äºè¿”å›åˆ—è¡¨é¡µ
            list_page_url = self.driver.current_url
            
            # æŸ¥æ‰¾å¹¶ç‚¹å‡» .title-area ä¸‹çš„ .title å…ƒç´ 
            try:
                title_element = element.find_element(By.CSS_SELECTOR, "span.title, .title")
                
                # æ»šåŠ¨åˆ°å…ƒç´ å¯è§
                self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", title_element)
                time.sleep(0.5)
                
                # ç‚¹å‡»æ ‡é¢˜è¿›å…¥è¯¦æƒ…é¡µ
                # åœ¨ç‚¹å‡»å‰ä¿å­˜åˆ—è¡¨é¡µURLï¼Œç”¨äºè¿”å›
                list_page_url = self.driver.current_url
                
                try:
                    # å°è¯•ç›´æ¥ç‚¹å‡»
                    title_element.click()
                except:
                    # å¦‚æœç›´æ¥ç‚¹å‡»å¤±è´¥ï¼Œä½¿ç”¨JavaScriptç‚¹å‡»
                    self.driver.execute_script("arguments[0].click();", title_element)
                
                # ç­‰å¾…é¡µé¢è·³è½¬åˆ°è¯¦æƒ…é¡µ
                # ç­‰å¾…è¶³å¤Ÿçš„æ—¶é—´è®©é¡µé¢è·³è½¬å’Œå†…å®¹åŠ è½½
                print("  ç­‰å¾…é¡µé¢è·³è½¬...")
                time.sleep(5)  # ç­‰å¾…é¡µé¢è·³è½¬
                
                # æ£€æŸ¥URLæ˜¯å¦æ”¹å˜ï¼ˆç”¨äºæç¤ºï¼Œä½†ä¸å½±å“åç»­æ“ä½œï¼‰
                current_url = self.driver.current_url
                if current_url != list_page_url:
                    print(f"  å·²è·³è½¬åˆ°è¯¦æƒ…é¡µ")
                else:
                    print(f"  ç»§ç»­ç­‰å¾…é¡µé¢åŠ è½½...")
                    time.sleep(3)  # å†ç­‰å¾…ä¸€ä¸‹
                
                # æ— è®ºURLæ˜¯å¦æ£€æµ‹åˆ°å˜åŒ–ï¼Œéƒ½ç»§ç»­å°è¯•æå–ï¼ˆå› ä¸ºç‚¹å‡»ååº”è¯¥å·²ç»è·³è½¬äº†ï¼‰
                
                # ä»è¯¦æƒ…é¡µæå–ä¿¡æ¯
                paper = self.extract_paper_from_detail_page()
                
                # è¿”å›åˆ—è¡¨é¡µ
                if list_page_url:
                    self.driver.get(list_page_url)
                    time.sleep(2)
                    
                    # ç­‰å¾…åˆ—è¡¨é¡µé‡æ–°åŠ è½½
                    try:
                        wait = WebDriverWait(self.driver, 10)
                        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".title-area")))
                    except TimeoutException:
                        pass
                
                return paper
                
            except NoSuchElementException:
                print("  æœªæ‰¾åˆ°æ ‡é¢˜å…ƒç´ ")
                return None
                
        except Exception as e:
            print(f"  ç‚¹å‡»æ ‡é¢˜è¿›å…¥è¯¦æƒ…é¡µæ—¶å‡ºé”™: {e}")
            # å°è¯•è¿”å›åˆ—è¡¨é¡µ
            try:
                if list_page_url:
                    self.driver.get(list_page_url)
                    time.sleep(2)
            except:
                pass
            return None

    def extract_papers_from_page(self):
        """ä»å½“å‰é¡µé¢æå–è®ºæ–‡ä¿¡æ¯"""
        papers = []
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡é¡¹
            # ä¸‡æ–¹æ•°æ®å¯èƒ½ä½¿ç”¨å¤šç§ç»“æ„ï¼Œå°è¯•ä¸åŒçš„é€‰æ‹©å™¨
            paper_elements = []
            
            # æ–¹æ³•1: æŸ¥æ‰¾ .title-area
            try:
                title_areas = self.driver.find_elements(By.CSS_SELECTOR, ".title-area")
                if title_areas:
                    paper_elements = title_areas
                    print(f"æ‰¾åˆ° {len(paper_elements)} ä¸ª .title-area å…ƒç´ ")
            except:
                pass
            
            # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
            if not paper_elements:
                try:
                    result_items = self.driver.find_elements(By.CSS_SELECTOR, ".result-item, [class*='result-item'], [class*='paper-item']")
                    if result_items:
                        paper_elements = result_items
                        print(f"æ‰¾åˆ° {len(paper_elements)} ä¸ªç»“æœé¡¹")
                except:
                    pass
            
            # æ–¹æ³•3: æŸ¥æ‰¾åŒ…å«æ ‡é¢˜çš„å®¹å™¨
            if not paper_elements:
                try:
                    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é“¾æ¥çš„å®¹å™¨
                    containers = self.driver.find_elements(By.CSS_SELECTOR, "div[class*='item'], div[class*='result'], div[class*='paper']")
                    paper_elements = [c for c in containers if c.find_elements(By.CSS_SELECTOR, ".title, a[href*='detail'], a[href*='paper']")]
                    print(f"æ‰¾åˆ° {len(paper_elements)} ä¸ªå¯èƒ½çš„è®ºæ–‡å®¹å™¨")
                except:
                    pass
            
            # ä¿å­˜åˆ—è¡¨é¡µURL
            list_page_url = self.driver.current_url
            
            # æå–æ¯ç¯‡è®ºæ–‡
            total_count = len(paper_elements)
            for i in range(total_count):
                try:
                    print(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{total_count} ç¯‡è®ºæ–‡...")
                    
                    # æ¯æ¬¡å¾ªç¯å‰é‡æ–°è·å–å…ƒç´ ï¼ˆå› ä¸ºé¡µé¢å¯èƒ½è¢«é‡æ–°åŠ è½½ï¼‰
                    try:
                        current_elements = self.driver.find_elements(By.CSS_SELECTOR, ".title-area")
                        if not current_elements:
                            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
                            current_elements = self.driver.find_elements(By.CSS_SELECTOR, ".result-item, [class*='result-item'], [class*='paper-item']")
                        
                        if i < len(current_elements):
                            element = current_elements[i]
                        else:
                            print(f"  ç¬¬ {i+1} ä¸ªå…ƒç´ ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                            continue
                    except Exception as e:
                        print(f"  é‡æ–°è·å–å…ƒç´ å¤±è´¥: {e}")
                        # å°è¯•é‡æ–°åŠ è½½åˆ—è¡¨é¡µ
                        try:
                            self.driver.get(list_page_url)
                            time.sleep(3)
                            current_elements = self.driver.find_elements(By.CSS_SELECTOR, ".title-area")
                            if i < len(current_elements):
                                element = current_elements[i]
                            else:
                                continue
                        except:
                            continue
                    
                    # æå–è®ºæ–‡ä¿¡æ¯
                    paper = self.extract_paper_from_element(element)
                    if paper and paper['title']:
                        papers.append(paper)
                        print(f"âœ… ç¬¬ {i+1} ç¯‡è®ºæ–‡æå–æˆåŠŸ: {paper['title'][:50]}...")
                    else:
                        print(f"âš ï¸ ç¬¬ {i+1} ç¯‡è®ºæ–‡æå–å¤±è´¥")
                        
                except Exception as e:
                    print(f"æå–ç¬¬ {i+1} ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {e}")
                    # ç¡®ä¿è¿”å›åˆ—è¡¨é¡µ
                    try:
                        if self.driver.current_url != list_page_url:
                            self.driver.get(list_page_url)
                            time.sleep(2)
                    except:
                        pass
                    continue
            
            return papers
            
        except Exception as e:
            print(f"æå–è®ºæ–‡ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def extract_paper_from_detail_page(self):
        """ä»è¯¦æƒ…é¡µæå–è®ºæ–‡ä¿¡æ¯"""
        paper = {
            'title': '',
            'authors': '',
            'journal': '',
            'publish_time': '',
            'link': ''
        }

        try:
            # æå–è®ºæ–‡åï¼ˆspan.detailTitleCNï¼‰
            try:
                title_element = self.driver.find_element(By.CSS_SELECTOR, "span.detailTitleCN")
                paper['title'] = title_element.text.strip()
            except NoSuchElementException:
                print("  æœªæ‰¾åˆ°è®ºæ–‡å")
                return None

            # æå–é“¾æ¥ï¼ˆå½“å‰URLï¼‰
            paper['link'] = self.driver.current_url

            # æå–ä½œè€…ï¼ˆdiv.author.detailTitle ä¸‹çš„ a æ ‡ç­¾çš„ span é‡Œï¼‰
            try:
                author_div = self.driver.find_element(By.CSS_SELECTOR, "div.author.detailTitle")
                author_links = author_div.find_elements(By.TAG_NAME, "a")
                
                authors_list = []
                for author_link in author_links:
                    try:
                        # æŸ¥æ‰¾ a æ ‡ç­¾å†…çš„ span
                        author_span = author_link.find_element(By.TAG_NAME, "span")
                        author_name = author_span.text.strip()
                        if author_name:
                            authors_list.append(author_name)
                    except NoSuchElementException:
                        # å¦‚æœæ²¡æœ‰spanï¼Œç›´æ¥å–aæ ‡ç­¾çš„æ–‡æœ¬
                        author_name = author_link.text.strip()
                        if author_name:
                            authors_list.append(author_name)
                
                paper['authors'] = ';'.join(authors_list)
            except NoSuchElementException:
                print("  æœªæ‰¾åˆ°ä½œè€…ä¿¡æ¯")
                paper['authors'] = ''

            # æå–æœŸåˆŠåï¼ˆclass="periodicalName" çš„ a æ ‡ç­¾é‡Œï¼‰
            try:
                journal_element = self.driver.find_element(By.CSS_SELECTOR, "a.periodicalName, .periodicalName a")
                paper['journal'] = journal_element.text.strip()
            except NoSuchElementException:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                try:
                    journal_element = self.driver.find_element(By.CSS_SELECTOR, "[class*='periodicalName'] a")
                    paper['journal'] = journal_element.text.strip()
                except:
                    print("  æœªæ‰¾åˆ°æœŸåˆŠå")
                    paper['journal'] = ''

            # æå–å‘è¡¨æ—¶é—´ï¼ˆclass="publish list" ä¸‹çš„ class="itemUrl"ï¼‰
            try:
                publish_list = self.driver.find_element(By.CSS_SELECTOR, ".publish.list")
                item_url = publish_list.find_element(By.CSS_SELECTOR, ".itemUrl")
                paper['publish_time'] = item_url.text.strip()
            except NoSuchElementException:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                try:
                    publish_list = self.driver.find_element(By.CSS_SELECTOR, "[class*='publish'][class*='list']")
                    item_url = publish_list.find_element(By.CSS_SELECTOR, "[class*='itemUrl']")
                    paper['publish_time'] = item_url.text.strip()
                except:
                    print("  æœªæ‰¾åˆ°å‘è¡¨æ—¶é—´")
                    paper['publish_time'] = ''

            # æ¸…ç†æ•°æ®
            for key in paper:
                if isinstance(paper[key], str):
                    paper[key] = re.sub(r'\s+', ' ', paper[key]).strip()

            return paper

        except Exception as e:
            print(f"  æå–è¯¦æƒ…é¡µä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None

    def filter_papers_by_year(self, papers, target_year):
        """è¿‡æ»¤è®ºæ–‡ï¼Œåªä¿ç•™æŒ‡å®šå¹´ä»½çš„è®ºæ–‡"""
        filtered = []
        for paper in papers:
            publish_time = paper.get('publish_time', '')
            if not publish_time:
                continue
            
            year_match = re.search(r'(19|20)\d{2}', str(publish_time))
            if year_match:
                year = int(year_match.group())
                if year == target_year:
                    filtered.append(paper)
        
        return filtered

    def scrape_papers(self, url, max_pages=1, target_year=None):
        """æŠ“å–è®ºæ–‡æ•°æ®"""
        if not self.setup_driver():
            return []
        
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®é“¾æ¥: {url}")
            self.driver.get(url)
            time.sleep(5)
            
            print(f"ğŸ“„ æ­£åœ¨æŠ“å–æ•°æ®...")
            
            all_papers = []
            if self.wait_for_results():
                papers = self.extract_papers_from_page()
                if papers:
                    all_papers = papers
                    print(f"âœ… æå–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å¹´ä»½ï¼Œè¿‡æ»¤æ•°æ®
            if target_year:
                print(f"ğŸ” æ­£åœ¨è¿‡æ»¤æ•°æ®ï¼Œåªä¿ç•™{target_year}å¹´çš„è®ºæ–‡...")
                all_papers = self.filter_papers_by_year(all_papers, target_year)
                print(f"âœ… è¿‡æ»¤åå‰©ä½™ {len(all_papers)} ç¯‡è®ºæ–‡")
            
            self.papers_data = all_papers
            return all_papers
            
        except Exception as e:
            print(f"æŠ“å–è¿‡ç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
        
        finally:
            if self.driver:
                self.driver.quit()

    def save_to_excel(self, papers, filename=None):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ä¸‡æ–¹æ•°æ®_{timestamp}.xlsx"
        
        if not papers:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            data = []
            for paper in papers:
                data.append({
                    'ä½œè€…': paper.get('authors', ''),
                    'è®ºæ–‡å': paper.get('title', ''),
                    'æœŸåˆŠå': paper.get('journal', ''),
                    'å‘è¡¨æ—¶é—´': paper.get('publish_time', ''),
                    'é“¾æ¥': paper.get('link', '')
                })
            
            df = pd.DataFrame(data)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
            
        except Exception as e:
            print(f"ä¿å­˜å¤±è´¥: {e}")
            return False


def build_url_with_year(base_url, year):
    """æ„å»ºåŒ…å«æŒ‡å®šå¹´ä»½çš„URL"""
    # å°†URLä¸­çš„Date:2024-2024æˆ–Date%3A2024-2024æ›¿æ¢ä¸ºæŒ‡å®šå¹´ä»½
    # æ³¨æ„ï¼šURLä¸­å¯èƒ½ä½¿ç”¨URLç¼–ç ï¼Œ%3Aæ˜¯å†’å·çš„ç¼–ç 
    new_url = base_url
    
    # å¤„ç†URLç¼–ç çš„æƒ…å†µï¼šDate%3A2024-2024
    if 'Date%3A' in new_url:
        new_url = re.sub(r'Date%3A\d{4}-\d{4}', f'Date%3A{year}-{year}', new_url)
    # å¤„ç†æœªç¼–ç çš„æƒ…å†µï¼šDate:2024-2024
    elif 'Date:' in new_url:
        new_url = re.sub(r'Date:\d{4}-\d{4}', f'Date:{year}-{year}', new_url)
    else:
        # å¦‚æœæ²¡æœ‰Dateå‚æ•°ï¼Œæ·»åŠ å®ƒï¼ˆä½¿ç”¨URLç¼–ç æ ¼å¼ï¼‰
        if '?' in new_url:
            new_url = f"{new_url}&Date%3A{year}-{year}"
        else:
            new_url = f"{new_url}?Date%3A{year}-{year}"
    
    return new_url


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¿è¡Œä¸‡æ–¹æ•°æ®æŠ“å–å·¥å…·...")
    
    # ä¸‡æ–¹æ•°æ®é“¾æ¥
    base_url = "https://s.wanfangdata.com.cn/paper?q=%28%E4%BD%9C%E8%80%85%E5%8D%95%E4%BD%8D%3A%E9%A6%96%E9%83%BD%E5%9B%BE%E4%B9%A6%E9%A6%86%29%20Date%3A2022-2022&p=1&s=100"
    
    # è·å–å»å¹´çš„å¹´ä»½
    current_year = datetime.now().year
    last_year = current_year - 1
    print(f"ğŸ“… ç›®æ ‡æ—¶é—´èŒƒå›´ï¼š{last_year}å¹´ï¼ˆå»å¹´ï¼‰")
    
    # æ„å»ºåŒ…å«å»å¹´å¹´ä»½çš„URL
    url = build_url_with_year(base_url, last_year)
    print(f"ğŸ”— ç›®æ ‡URL: {url}")
    
    try:
        scraper = WanfangDataScraper()
        print(f"ğŸ” å¼€å§‹æŠ“å–{last_year}å¹´çš„è®ºæ–‡æ•°æ®...")
        papers = scraper.scrape_papers(url, max_pages=10, target_year=last_year)
        
        if papers:
            print(f"âœ… æˆåŠŸæŠ“å–åˆ° {len(papers)} ç¯‡{last_year}å¹´çš„è®ºæ–‡")
            print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°Excelæ–‡ä»¶...")
            scraper.save_to_excel(papers, filename=f"ä¸‡æ–¹æ•°æ®_{last_year}å¹´.xlsx")
            print("ğŸ‰ æŠ“å–å®Œæˆï¼")
        else:
            print(f"âŒ æ²¡æœ‰æŠ“å–åˆ°{last_year}å¹´çš„æ•°æ®")
    
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

