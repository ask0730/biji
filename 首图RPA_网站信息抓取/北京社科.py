import time
import re
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import TimeoutException, NoSuchElementException

try:
    from docx import Document
    from docx.shared import Pt, RGBColor, Inches
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    print("âš ï¸ æœªå®‰è£…python-docxåº“ï¼ŒWordä¿å­˜åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install python-docx")

class BJSKScraper:
    def __init__(self):
        self.driver = None
        self.articles_data = []
    
    def parse_date(self, date_str):
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œè¿”å›datetimeå¯¹è±¡"""
        if not date_str:
            return None
        
        try:
            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼ï¼ˆåŒ—äº¬ç¤¾ç§‘ä½¿ç”¨ 2025.10.31 æ ¼å¼ï¼‰
            date_formats = [
                '%Y.%m.%d',  # åŒ—äº¬ç¤¾ç§‘æ ¼å¼ï¼š2025.10.31
                '%Y-%m-%d',
                '%Yå¹´%mæœˆ%dæ—¥',
                '%Y/%m/%d',
                '%Y-%m-%d %H:%M:%S',
                '%Y/%m/%d %H:%M:%S',
                '%Y.%m.%d %H:%M:%S'
            ]
            
            for fmt in date_formats:
                try:
                    return datetime.strptime(date_str, fmt)
                except:
                    continue
            
            # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œå°è¯•æ­£åˆ™æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆæ”¯æŒç‚¹åˆ†éš”ï¼‰
            date_match = re.search(r'(\d{4})[.\-\/å¹´](\d{1,2})[.\-\/æœˆ](\d{1,2})', date_str)
            if date_match:
                year, month, day = date_match.groups()
                return datetime(int(year), int(month), int(day))
            
            return None
        except:
            return None
    
    def filter_articles_by_date_range(self, articles, start_date=None, end_date=None, days=None):
        """æ ¹æ®æ—¥æœŸèŒƒå›´è¿‡æ»¤æ–‡ç« """
        if not articles:
            return []
        
        # å¦‚æœæŒ‡å®šäº†å¤©æ•°ï¼Œè®¡ç®—å¼€å§‹æ—¥æœŸ
        if days is not None:
            start_date = datetime.now() - timedelta(days=days)
            end_date = datetime.now()
        elif start_date is None:
            # é»˜è®¤ä½¿ç”¨æœ€è¿‘7å¤©
            start_date = datetime.now() - timedelta(days=7)
            end_date = datetime.now()
        
        # ç¡®ä¿start_dateå’Œend_dateæ˜¯datetimeå¯¹è±¡
        if isinstance(start_date, str):
            start_date = self.parse_date(start_date)
        if isinstance(end_date, str):
            end_date = self.parse_date(end_date)
        
        if start_date:
            start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
        if end_date:
            end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=999999)
        
        filtered = []
        
        for article in articles:
            publish_time = article.get('publish_time', '')
            if not publish_time:
                # å¦‚æœæ²¡æœ‰å‘å¸ƒæ—¶é—´ï¼Œå°è¯•ä»é“¾æ¥ä¸­æå–æ—¥æœŸ
                link = article.get('link', '')
                if link:
                    # å°è¯•ä»URLä¸­æå–æ—¥æœŸï¼ˆåŒ—äº¬ç¤¾ç§‘å¯èƒ½ä½¿ç”¨ä¸åŒçš„æ—¥æœŸæ ¼å¼ï¼‰
                    date_match = re.search(r'/(\d{4})[/-](\d{2})[/-](\d{2})/', link)
                    if date_match:
                        year, month, day = date_match.groups()
                        publish_time = f"{year}-{month}-{day}"
                        article['publish_time'] = publish_time
            
            if publish_time:
                article_date = self.parse_date(publish_time)
                if article_date:
                    article_date = article_date.replace(hour=0, minute=0, second=0, microsecond=0)
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ—¥æœŸèŒƒå›´å†…
                    if start_date and article_date < start_date:
                        continue
                    if end_date and article_date > end_date:
                        continue
                    filtered.append(article)
                else:
                    # å¦‚æœæ— æ³•è§£ææ—¥æœŸï¼Œé»˜è®¤åŒ…å«ï¼ˆé¿å…é—æ¼ï¼‰
                    print(f"  âš ï¸ æ–‡ç«  '{article.get('title', '')}' æ—¥æœŸæ ¼å¼æ— æ³•è§£æï¼Œå°†åŒ…å«åœ¨å†…")
                    filtered.append(article)
            else:
                # å¦‚æœæ²¡æœ‰æ—¥æœŸä¿¡æ¯ï¼Œé»˜è®¤åŒ…å«ï¼ˆé¿å…é—æ¼ï¼‰
                print(f"  âš ï¸ æ–‡ç«  '{article.get('title', '')}' æ²¡æœ‰æ—¥æœŸä¿¡æ¯ï¼Œå°†åŒ…å«åœ¨å†…")
                filtered.append(article)
        
        return filtered
    
    def load_config(self, config_file="config.txt"):
        """ä»é…ç½®æ–‡ä»¶è¯»å–è®¾ç½®"""
        config = {
            'days': 7,  # é»˜è®¤7å¤©
            'start_date': None,
            'end_date': None,
            'output_dir': 'é¡¹ç›®ç”³æŠ¥æ–‡ç« '  # é»˜è®¤è¾“å‡ºç›®å½•
        }
        
        try:
            import os
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                        if not line or line.startswith('#'):
                            continue
                        
                        # è§£æé…ç½®é¡¹
                        if '=' in line:
                            key, value = line.split('=', 1)
                            key = key.strip()
                            value = value.strip()
                            
                            if key == 'days':
                                try:
                                    config['days'] = int(value)
                                except:
                                    pass
                            elif key == 'start_date':
                                config['start_date'] = value
                            elif key == 'end_date':
                                config['end_date'] = value
                            elif key == 'output_dir':
                                config['output_dir'] = value
        except Exception as e:
            print(f"âš ï¸ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®ï¼ˆæœ€è¿‘7å¤©ï¼‰")
        
        return config

    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(10)
            return True
            
        except Exception as e:
            print(f"æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            return False

    def search_articles_by_keyword(self, keyword="é¡¹ç›®ç”³æŠ¥"):
        """æœç´¢åŒ…å«å…³é”®è¯çš„æ–‡ç« """
        articles = []
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            
            # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ï¼Œç­›é€‰åŒ…å«å…³é”®è¯çš„æ ‡é¢˜
            all_links = self.driver.find_elements(By.TAG_NAME, "a")
            
            print(f"ğŸ” æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥ï¼Œæ­£åœ¨ç­›é€‰åŒ…å«'{keyword}'çš„æ–‡ç« ...")
            
            for link in all_links:
                try:
                    title = link.text.strip()
                    href = link.get_attribute('href')
                    
                    # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯
                    if title and keyword in title and len(title) > 3:
                        # è·å–é“¾æ¥çš„å®Œæ•´URL
                        if href:
                            if href.startswith('/'):
                                # ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æ‹¼æ¥
                                current_url = self.driver.current_url
                                base_url = '/'.join(current_url.split('/')[:3])
                                full_url = base_url + href
                            elif href.startswith('http'):
                                full_url = href
                            else:
                                full_url = href
                        else:
                            full_url = ''
                        
                        # å°è¯•è·å–å‘å¸ƒæ—¶é—´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        publish_time = ''
                        try:
                            # æŸ¥æ‰¾çˆ¶å…ƒç´ æˆ–å…„å¼Ÿå…ƒç´ ä¸­çš„æ—¥æœŸ
                            parent = link.find_element(By.XPATH, "./..")
                            parent_text = parent.text
                            
                            # å°è¯•æå–æ—¥æœŸæ ¼å¼ï¼ˆä¼˜å…ˆæ”¯æŒåŒ—äº¬ç¤¾ç§‘æ ¼å¼ï¼š2025.10.31ï¼‰
                            date_patterns = [
                                r'(\d{4}\.\d{1,2}\.\d{1,2})',  # åŒ—äº¬ç¤¾ç§‘æ ¼å¼ï¼š2025.10.31
                                r'(\d{4}-\d{1,2}-\d{1,2})',
                                r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                                r'(\d{4}/\d{1,2}/\d{1,2})'
                            ]
                            
                            for pattern in date_patterns:
                                match = re.search(pattern, parent_text)
                                if match:
                                    publish_time = match.group(1)
                                    break
                        except:
                            pass
                        
                        article = {
                            'title': title,
                            'link': full_url,
                            'publish_time': publish_time
                        }
                        
                        # é¿å…é‡å¤
                        if article not in articles:
                            articles.append(article)
                            print(f"âœ… æ‰¾åˆ°æ–‡ç« : {title}")
                
                except Exception as e:
                    continue
            
            # æ–¹æ³•2: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ–‡æœ¬çš„å…ƒç´ 
            try:
                # ä½¿ç”¨XPathæŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ–‡æœ¬èŠ‚ç‚¹
                xpath_query = f"//*[contains(text(), '{keyword}')]"
                matching_elements = self.driver.find_elements(By.XPATH, xpath_query)
                
                for element in matching_elements:
                    try:
                        text = element.text.strip()
                        tag_name = element.tag_name.lower()
                        
                        # å¦‚æœæ˜¯é“¾æ¥ï¼Œç›´æ¥æå–
                        if tag_name == 'a':
                            title = text
                            href = element.get_attribute('href')
                            
                            if title and keyword in title and len(title) > 3:
                                if href:
                                    if href.startswith('/'):
                                        current_url = self.driver.current_url
                                        base_url = '/'.join(current_url.split('/')[:3])
                                        full_url = base_url + href
                                    elif href.startswith('http'):
                                        full_url = href
                                    else:
                                        full_url = href
                                else:
                                    full_url = ''
                                
                                article = {
                                    'title': title,
                                    'link': full_url,
                                    'publish_time': ''
                                }
                                
                                # é¿å…é‡å¤
                                if article not in articles:
                                    articles.append(article)
                                    print(f"âœ… æ‰¾åˆ°æ–‡ç« : {title}")
                        
                        # å¦‚æœä¸æ˜¯é“¾æ¥ï¼Œå°è¯•æŸ¥æ‰¾çˆ¶çº§æˆ–å­çº§é“¾æ¥
                        else:
                            try:
                                # å°è¯•åœ¨çˆ¶å…ƒç´ ä¸­æŸ¥æ‰¾é“¾æ¥
                                parent = element.find_element(By.XPATH, "./..")
                                parent_links = parent.find_elements(By.TAG_NAME, "a")
                                
                                for parent_link in parent_links:
                                    link_text = parent_link.text.strip()
                                    if keyword in link_text and len(link_text) > 3:
                                        href = parent_link.get_attribute('href')
                                        
                                        if href:
                                            if href.startswith('/'):
                                                current_url = self.driver.current_url
                                                base_url = '/'.join(current_url.split('/')[:3])
                                                full_url = base_url + href
                                            elif href.startswith('http'):
                                                full_url = href
                                            else:
                                                full_url = href
                                        else:
                                            full_url = ''
                                        
                                        article = {
                                            'title': link_text,
                                            'link': full_url,
                                            'publish_time': ''
                                        }
                                        
                                        if article not in articles:
                                            articles.append(article)
                                            print(f"âœ… æ‰¾åˆ°æ–‡ç« : {link_text}")
                            except:
                                pass
                    except:
                        continue
            except Exception as e:
                print(f"XPathæœç´¢æ—¶å‡ºé”™: {e}")
            
            return articles
            
        except Exception as e:
            print(f"æœç´¢æ–‡ç« å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def extract_article_content(self, article_url):
        """è®¿é—®æ–‡ç« é“¾æ¥å¹¶æå–æ–‡ç« å†…å®¹å’Œå‘å¸ƒæ—¥æœŸ"""
        try:
            print(f"  ğŸ“„ æ­£åœ¨è®¿é—®æ–‡ç« : {article_url}")
            self.driver.get(article_url)
            # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
            time.sleep(5)
            
            content = ""
            publish_date = ""
            article_title = ""  # ä»è¯¦æƒ…é¡µæå–æ ‡é¢˜
            
            # å°è¯•ä»è¯¦æƒ…é¡µæå–æ ‡é¢˜ï¼ˆæ›´å‡†ç¡®ï¼‰
            try:
                # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
                title_selectors = [
                    "h1",
                    "h2.title",
                    ".title",
                    "[class*='title']",
                    "div.title",
                    "article h1",
                    "article h2"
                ]
                
                for selector in title_selectors:
                    try:
                        title_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        for title_elem in title_elements:
                            title_text = title_elem.text.strip()
                            if title_text and len(title_text) > 5:  # è‡³å°‘5ä¸ªå­—ç¬¦
                                article_title = title_text
                                print(f"  ğŸ“Œ ä»è¯¦æƒ…é¡µæå–åˆ°æ ‡é¢˜: {article_title}")
                                break
                        if article_title:
                            break
                    except:
                        continue
            except:
                pass
            
            # å°è¯•ä»URLä¸­æå–æ—¥æœŸ
            date_match = re.search(r'/(\d{4})[/-](\d{2})[/-](\d{2})/', article_url)
            if date_match:
                year, month, day = date_match.groups()
                publish_date = f"{year}-{month}-{day}"
            
            # å°è¯•ä»é¡µé¢ä¸­æå–å‘å¸ƒæ—¥æœŸ
            date_selectors = [
                "span.publish-time",
                "div.publish-time",
                "span.date",
                "div.date",
                "time",
                "[class*='date']",
                "[class*='time']",
                "[class*='publish']"
            ]
            
            for selector in date_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if text and re.search(r'\d{4}', text):
                            parsed_date = self.parse_date(text)
                            if parsed_date:
                                publish_date = parsed_date.strftime("%Y-%m-%d")
                                break
                    if publish_date:
                        break
                except:
                    continue
            
            # å…ˆç§»é™¤scriptå’Œstyleæ ‡ç­¾ï¼Œé¿å…å¹²æ‰°
            try:
                self.driver.execute_script("""
                    var scripts = document.getElementsByTagName('script');
                    for(var i = scripts.length - 1; i >= 0; i--) {
                        scripts[i].parentNode.removeChild(scripts[i]);
                    }
                    var styles = document.getElementsByTagName('style');
                    for(var i = styles.length - 1; i >= 0; i--) {
                        styles[i].parentNode.removeChild(styles[i]);
                    }
                    var navs = document.getElementsByTagName('nav');
                    for(var i = navs.length - 1; i >= 0; i--) {
                        navs[i].style.display = 'none';
                    }
                    var footers = document.getElementsByTagName('footer');
                    for(var i = footers.length - 1; i >= 0; i--) {
                        footers[i].style.display = 'none';
                    }
                    var headers = document.querySelectorAll('header, .header, #header');
                    for(var i = 0; i < headers.length; i++) {
                        headers[i].style.display = 'none';
                    }
                """)
                time.sleep(1)
            except:
                pass
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥å®šä½æ–‡ç« å†…å®¹ï¼ˆåŒ—äº¬ç¤¾ç§‘ç½‘ç«™ï¼‰
            content_selectors = [
                "div.content",
                "div.article-content",
                "div.article-body",
                "div.text",
                "div.main-content",
                "div.detail-content",
                "div.news-content",
                "article",
                "div[class*='content']",
                "div[class*='article']",
                "div[class*='detail']",
                "div[class*='text']",
                "#content",
                "#article",
                "#detail",
                ".content",
                ".article",
                ".detail"
            ]
            
            for selector in content_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        # é€‰æ‹©æœ€é•¿çš„æ–‡æœ¬å†…å®¹ï¼ˆé€šå¸¸æ˜¯æ­£æ–‡ï¼‰
                        for element in elements:
                            text = element.text.strip()
                            if len(text) > len(content) and len(text) > 200:
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¾ƒå¤šä¸­æ–‡
                                chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', text))
                                if chinese_chars > 50:
                                    content = text
                        if content:
                            break
                except:
                    continue
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«æœ€å¤šæ–‡æœ¬çš„div
            if not content or len(content) < 200:
                try:
                    divs = self.driver.find_elements(By.TAG_NAME, "div")
                    best_div = None
                    best_score = 0
                    
                    for div in divs:
                        try:
                            text = div.text.strip()
                            if len(text) > 500:
                                chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', text))
                                if 'æœªèƒ½æå–' not in text and 'æå–å¤±è´¥' not in text:
                                    score = chinese_chars * 0.7 + len(text) * 0.3
                                    if score > best_score and chinese_chars > 100:
                                        best_score = score
                                        best_div = div
                        except:
                            continue
                    
                    if best_div:
                        content = best_div.text.strip()
                        print(f"  ğŸ“ é€šè¿‡divéå†æ‰¾åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
                except Exception as e:
                    print(f"  âš ï¸ divæŸ¥æ‰¾å¤±è´¥: {e}")
            
            if content:
                print(f"  âœ… æˆåŠŸæå–å†…å®¹ï¼Œé•¿åº¦: {len(content)}å­—ç¬¦")
            else:
                print(f"  âš ï¸ æœªèƒ½æå–åˆ°æ–‡ç« å†…å®¹")
            
            return content, publish_date, article_title
            
        except Exception as e:
            print(f"  âš ï¸ æå–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return "", "", ""

    def scrape_articles(self, url, keyword="é¡¹ç›®ç”³æŠ¥", extract_content=True, max_articles=None):
        """æŠ“å–åŒ…å«å…³é”®è¯çš„æ–‡ç« """
        if not self.setup_driver():
            return []
        
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
            self.driver.get(url)
            time.sleep(5)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            print(f"ğŸ” æ­£åœ¨æœç´¢åŒ…å«'{keyword}'çš„æ–‡ç« ...")
            articles = self.search_articles_by_keyword(keyword)
            
            # å¦‚æœè®¾ç½®äº†æœ€å¤§æ–‡ç« æ•°ï¼Œåªå¤„ç†å‰Nç¯‡
            if max_articles and len(articles) > max_articles:
                articles = articles[:max_articles]
                print(f"ğŸ“Œ ä»…å¤„ç†å‰ {max_articles} ç¯‡æ–‡ç« ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
            
            # å¦‚æœè®¾ç½®äº†æå–å†…å®¹ï¼Œè®¿é—®æ¯ä¸ªæ–‡ç« é“¾æ¥å¹¶æå–å†…å®¹
            if extract_content and articles:
                print(f"\nğŸ“– å¼€å§‹æå– {len(articles)} ç¯‡æ–‡ç« çš„å†…å®¹...")
                for i, article in enumerate(articles, 1):
                    if article.get('link'):
                        print(f"\n[{i}/{len(articles)}] æ­£åœ¨æå–æ–‡ç« å†…å®¹...")
                        content, publish_date, extracted_title = self.extract_article_content(article['link'])
                        article['content'] = content
                        # å¦‚æœä»è¯¦æƒ…é¡µæå–åˆ°äº†æ ‡é¢˜ï¼Œæ›´æ–°æ ‡é¢˜ï¼ˆæ›´å‡†ç¡®ï¼‰
                        if extracted_title:
                            article['title'] = extracted_title
                            print(f"  ğŸ“Œ æ›´æ–°æ ‡é¢˜: {extracted_title}")
                        # å¦‚æœæå–åˆ°äº†æ—¥æœŸä¸”åŸæ¥æ²¡æœ‰æ—¥æœŸï¼Œåˆ™æ›´æ–°
                        if publish_date and not article.get('publish_time'):
                            article['publish_time'] = publish_date
                            print(f"  ğŸ“… æå–åˆ°å‘å¸ƒæ—¥æœŸ: {publish_date}")
                        if content:
                            print(f"  âœ… æˆåŠŸæå–å†…å®¹ï¼ˆ{len(content)} å­—ç¬¦ï¼‰")
                        else:
                            print(f"  âš ï¸ æœªèƒ½æå–åˆ°å†…å®¹")
                        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            self.articles_data = articles
            return articles
            
        except Exception as e:
            print(f"æŠ“å–è¿‡ç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
        
        # ä¸åœ¨è¿™é‡Œå…³é—­æµè§ˆå™¨ï¼Œè®©è°ƒç”¨è€…å†³å®šä½•æ—¶å…³é—­

    def save_article_to_word(self, article, output_dir="."):
        """å°†å•ç¯‡æ–‡ç« ä¿å­˜ä¸ºå•ç‹¬çš„Wordæ–‡æ¡£"""
        if not DOCX_AVAILABLE:
            print("âŒ python-docxåº“æœªå®‰è£…ï¼Œæ— æ³•ä¿å­˜Wordæ–‡æ¡£")
            print("   è¯·è¿è¡Œ: pip install python-docx")
            return False
        
        if not article:
            return False
        
        try:
            # åˆ›å»ºWordæ–‡æ¡£
            doc = Document()
            
            # æ–‡ç« æ ‡é¢˜
            title = article.get("title", "æ— æ ‡é¢˜")
            # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦ï¼ˆWindowsæ–‡ä»¶åä¸å…è®¸çš„å­—ç¬¦ï¼‰
            # ç§»é™¤æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ç­‰ç©ºç™½å­—ç¬¦
            safe_title = title.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
            # ç§»é™¤Windowsæ–‡ä»¶åä¸å…è®¸çš„å­—ç¬¦
            safe_title = re.sub(r'[<>:"/\\|?*\n\r\t]', '_', safe_title)
            # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
            safe_title = re.sub(r'\s+', ' ', safe_title).strip()
            if len(safe_title) > 50:  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
                safe_title = safe_title[:50]
            
            article_title = doc.add_heading(title, 0)
            article_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # æ·»åŠ åˆ†éš”çº¿
            doc.add_paragraph('â”€' * 50)
            
            # æ–‡ç« å†…å®¹
            content = article.get('content', '')
            if content:
                # å°†å†…å®¹æŒ‰æ®µè½åˆ†å‰²
                paragraphs = content.split('\n')
                for para_text in paragraphs:
                    para_text = para_text.strip()
                    if para_text:  # åªæ·»åŠ éç©ºæ®µè½
                        para = doc.add_paragraph(para_text)
                        # è®¾ç½®æ®µè½æ ¼å¼
                        para_format = para.paragraph_format
                        para_format.space_after = Pt(6)
                        para_format.first_line_indent = Inches(0.25)  # é¦–è¡Œç¼©è¿›
            else:
                doc.add_paragraph('ï¼ˆæœªèƒ½æå–åˆ°æ–‡ç« å†…å®¹ï¼‰', style='Intense Quote')
            
            # æ–‡ç« é“¾æ¥ï¼ˆæ”¾åœ¨æœ€åï¼‰
            if article.get('link'):
                doc.add_paragraph('â”€' * 50)  # æ·»åŠ åˆ†éš”çº¿
                link_para = doc.add_paragraph(f'é“¾æ¥: {article["link"]}')
                # è®¾ç½®å­—ä½“å¤§å°
                if link_para.runs:
                    link_para.runs[0].font.size = Pt(10)
                link_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{safe_title}_{timestamp}.docx"
            # ä½¿ç”¨os.path.joinç¡®ä¿è·¯å¾„æ­£ç¡®ï¼ˆæ”¯æŒWindowså’ŒLinuxï¼‰
            import os
            filepath = os.path.join(output_dir, filename)
            
            # ä¿å­˜æ–‡æ¡£
            doc.save(filepath)
            print(f"  âœ… å·²ä¿å­˜: {filename}")
            return filepath
            
        except Exception as e:
            print(f"  âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def save_articles_to_word(self, articles, output_dir="."):
        """ä¸ºæ¯ç¯‡æ–‡ç« åˆ›å»ºå•ç‹¬çš„Wordæ–‡æ¡£"""
        if not DOCX_AVAILABLE:
            print("âŒ python-docxåº“æœªå®‰è£…ï¼Œæ— æ³•ä¿å­˜Wordæ–‡æ¡£")
            print("   è¯·è¿è¡Œ: pip install python-docx")
            return False
        
        if not articles:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return []
        
        import os
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        saved_files = []
        print(f"\nğŸ’¾ å¼€å§‹ä¿å­˜ {len(articles)} ç¯‡æ–‡ç« åˆ°Wordæ–‡æ¡£...")
        
        for i, article in enumerate(articles, 1):
            print(f"\n[{i}/{len(articles)}] æ­£åœ¨ä¿å­˜: {article.get('title', 'æ— æ ‡é¢˜')}")
            filepath = self.save_article_to_word(article, output_dir)
            if filepath:
                saved_files.append(filepath)
        
        print(f"\nâœ… å…±ä¿å­˜ {len(saved_files)} ä¸ªWordæ–‡æ¡£åˆ°ç›®å½•: {output_dir}")
        return saved_files

def main():
    print("ğŸš€ å¼€å§‹è¿è¡ŒåŒ—äº¬ç¤¾ç§‘æ–‡ç« æŠ“å–å·¥å…·...")
    url = "https://www.bjsk.org.cn/newslist-1486-0-0.html"
    keyword = "é¡¹ç›®ç”³æŠ¥"

    try:
        # è¯»å–é…ç½®æ–‡ä»¶
        scraper = BJSKScraper()
        config = scraper.load_config("config.txt")
        
        print("ğŸ“‹ é…ç½®ä¿¡æ¯:")
        if config.get('start_date') and config.get('end_date'):
            print(f"   æ—¥æœŸèŒƒå›´: {config['start_date']} è‡³ {config['end_date']}")
        else:
            print(f"   å¤©æ•°è®¾ç½®: æœ€è¿‘ {config['days']} å¤©")
        
        print("\nğŸ“± æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        print("ğŸ” å¼€å§‹æŠ“å–æ–‡ç« æ•°æ®...")
        # å…ˆåªæœç´¢æ–‡ç« ï¼Œä¸æå–å†…å®¹ï¼ˆèŠ‚çœæ—¶é—´ï¼‰
        articles = scraper.scrape_articles(url, keyword, extract_content=False)
        
        # æ ¹æ®é…ç½®æ–‡ä»¶è¿‡æ»¤æ–‡ç« ï¼ˆå…ˆä»URLä¸­æå–æ—¥æœŸè¿›è¡Œåˆæ­¥è¿‡æ»¤ï¼‰
        if articles:
            print(f"\nğŸ“… æ­£åœ¨æ ¹æ®é…ç½®è¿‡æ»¤æ–‡ç« ...")
            print(f"   åŸå§‹æ–‡ç« æ•°: {len(articles)}")
            
            # å…ˆä»URLä¸­æå–æ—¥æœŸï¼Œç”¨äºåˆæ­¥è¿‡æ»¤
            for article in articles:
                if not article.get('publish_time'):
                    link = article.get('link', '')
                    if link:
                        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                        date_match = re.search(r'/(\d{4})[/-](\d{2})[/-](\d{2})/', link)
                        if date_match:
                            year, month, day = date_match.groups()
                            article['publish_time'] = f"{year}-{month}-{day}"
            
            # æ ¹æ®é…ç½®è¿›è¡Œè¿‡æ»¤
            if config.get('start_date') and config.get('end_date'):
                articles = scraper.filter_articles_by_date_range(
                    articles, 
                    start_date=config['start_date'], 
                    end_date=config['end_date']
                )
                print(f"   è¿‡æ»¤åæ–‡ç« æ•°: {len(articles)} (æ—¥æœŸèŒƒå›´: {config['start_date']} è‡³ {config['end_date']})")
            else:
                articles = scraper.filter_articles_by_date_range(
                    articles, 
                    days=config['days']
                )
                print(f"   è¿‡æ»¤åæ–‡ç« æ•°: {len(articles)} (æœ€è¿‘{config['days']}å¤©)")
            
            # åªå¯¹è¿‡æ»¤åçš„æ–‡ç« æå–å†…å®¹
            if articles:
                print(f"\nğŸ“– å¼€å§‹æå– {len(articles)} ç¯‡è¿‡æ»¤åæ–‡ç« çš„å†…å®¹...")
                for i, article in enumerate(articles, 1):
                    if article.get('link'):
                        print(f"\n[{i}/{len(articles)}] æ­£åœ¨æå–æ–‡ç« å†…å®¹...")
                        content, publish_date, extracted_title = scraper.extract_article_content(article['link'])
                        article['content'] = content
                        # å¦‚æœä»è¯¦æƒ…é¡µæå–åˆ°äº†æ ‡é¢˜ï¼Œæ›´æ–°æ ‡é¢˜ï¼ˆæ›´å‡†ç¡®ï¼‰
                        if extracted_title:
                            article['title'] = extracted_title
                            print(f"  ğŸ“Œ æ›´æ–°æ ‡é¢˜: {extracted_title}")
                        # å¦‚æœæå–åˆ°äº†æ—¥æœŸä¸”åŸæ¥æ²¡æœ‰æ—¥æœŸï¼Œåˆ™æ›´æ–°
                        if publish_date and not article.get('publish_time'):
                            article['publish_time'] = publish_date
                            print(f"  ğŸ“… æå–åˆ°å‘å¸ƒæ—¥æœŸ: {publish_date}")
                        if content:
                            print(f"  âœ… æˆåŠŸæå–å†…å®¹ï¼ˆ{len(content)} å­—ç¬¦ï¼‰")
                        else:
                            print(f"  âš ï¸ æœªèƒ½æå–åˆ°å†…å®¹")
                        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«

        if articles:
            print(f"\nâœ… æˆåŠŸæŠ“å–åˆ° {len(articles)} ç¯‡åŒ…å«'{keyword}'çš„æ–‡ç« ")
            print("\nğŸ“‹ æ–‡ç« åˆ—è¡¨:")
            for i, article in enumerate(articles, 1):
                print(f"{i}. {article['title']}")
                if article['link']:
                    print(f"   é“¾æ¥: {article['link']}")
                if article['publish_time']:
                    print(f"   å‘å¸ƒæ—¶é—´: {article['publish_time']}")
                if article.get('content'):
                    content_preview = article['content'][:100] + "..." if len(article['content']) > 100 else article['content']
                    print(f"   å†…å®¹é¢„è§ˆ: {content_preview}")
                print()
            
            print("\nğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°Wordæ–‡æ¡£ï¼ˆæ¯ç¯‡æ–‡ç« å•ç‹¬ä¿å­˜ï¼‰...")
            import os
            output_dir = config.get('output_dir', 'é¡¹ç›®ç”³æŠ¥æ–‡ç« ')
            print(f"   è¾“å‡ºç›®å½•: {output_dir}")
            saved_files = scraper.save_articles_to_word(articles, output_dir)
            print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼å…±ä¿å­˜ {len(saved_files)} ä¸ªWordæ–‡æ¡£")
        else:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å«'{keyword}'çš„æ–‡ç« ")
        
        # æ‰€æœ‰æ“ä½œå®Œæˆåï¼Œå…³é—­æµè§ˆå™¨
        if scraper.driver:
            print("\nâ³ æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€5ç§’ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹ç»“æœ...")
            time.sleep(5)
            scraper.driver.quit()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        # å‡ºé”™æ—¶ä¹Ÿè¦å…³é—­æµè§ˆå™¨
        if 'scraper' in locals() and scraper.driver:
            scraper.driver.quit()

if __name__ == "__main__":
    main()

