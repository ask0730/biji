#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‡æ–¹æ•°æ®ç‹¬ç«‹ä½œè€…å…ƒç´ çˆ¬è™«
ä¸“é—¨å¤„ç†æ¯ä¸ªä½œè€…éƒ½æ˜¯ç‹¬ç«‹spanå…ƒç´ çš„æƒ…å†µ
"""

import time
import pandas as pd
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class IndependentAuthorWanfangScraper:
    def __init__(self):
        self.driver = None
        self.papers_data = []

    def setup_driver(self):
        """è®¾ç½®æµè§ˆå™¨é©±åŠ¨"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.implicitly_wait(5)
            return True
            
        except Exception as e:
            print(f"æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def wait_for_page_load(self, timeout=30):
        """ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ"""
        try:
            wait = WebDriverWait(self.driver, timeout)
            
            # ç­‰å¾…å…³é”®å…ƒç´ åŠ è½½
            selectors = [
                ".title-area",  # æ ‡é¢˜åŒºåŸŸ
                ".author-area",  # ä½œè€…åŒºåŸŸ
                ".right-content",  # ä¸»è¦å†…å®¹åŒºåŸŸ
                "body"  # ç¡®ä¿bodyåŠ è½½
            ]
            
            for selector in selectors:
                try:
                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                    print(f"é¡µé¢å…ƒç´ åŠ è½½å®Œæˆ: {selector}")
                    break
                except TimeoutException:
                    continue
            
            # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
            time.sleep(5)
            return True
            
        except Exception as e:
            print(f"ç­‰å¾…é¡µé¢åŠ è½½å¤±è´¥: {e}")
            return False
    
    def extract_authors_from_author_area(self, author_area):
        """ä»author-areaä¸­æå–æ‰€æœ‰ç‹¬ç«‹ä½œè€…"""
        authors_list = []
        
        try:
            print("ğŸ” æŸ¥æ‰¾ç‹¬ç«‹ä½œè€…å…ƒç´ ...")
            
            # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰ .authors å…ƒç´ ï¼ˆæ¯ä¸ªä½œè€…ä¸€ä¸ªç‹¬ç«‹çš„spanï¼‰
            authors_elements = author_area.find_elements(By.CSS_SELECTOR, ".authors")
            
            if authors_elements:
                print(f"æ‰¾åˆ° {len(authors_elements)} ä¸ªç‹¬ç«‹çš„ä½œè€…å…ƒç´ ")
                for i, element in enumerate(authors_elements):
                    author_text = element.text.strip()
                    if author_text and len(author_text) >= 2:
                        # è¿‡æ»¤æ‰æ—¶é—´ä¿¡æ¯ï¼ˆåŒ…å«å¹´ä»½çš„å†…å®¹ï¼‰
                        if re.search(r'(19|20)\d{2}.*æœŸ|å¹´|æœˆ|æ—¥', author_text):
                            print(f"  è·³è¿‡æ—¶é—´ä¿¡æ¯: {author_text}")
                            continue
                        
                        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯ä½œè€…åçš„å†…å®¹
                        skip_keywords = ['æœŸ', 'å¹´', 'æœˆ', 'æ—¥', 'å·', 'é¡µ', 'æ¥æº', 'æœŸåˆŠ', 'å‘è¡¨']
                        if any(keyword in author_text for keyword in skip_keywords):
                            print(f"  è·³è¿‡éä½œè€…å†…å®¹: {author_text}")
                            continue
                        
                        authors_list.append(author_text)
                        print(f"  ä½œè€… {i+1}: {author_text}")
            
            # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°ç‹¬ç«‹çš„ä½œè€…å…ƒç´ ï¼Œå°è¯•æŸ¥æ‰¾å•ä¸ª .authors å…ƒç´ 
            if not authors_list:
                print("æœªæ‰¾åˆ°ç‹¬ç«‹ä½œè€…å…ƒç´ ï¼Œå°è¯•æŸ¥æ‰¾å•ä¸ªä½œè€…å…ƒç´ ...")
                try:
                    authors_element = author_area.find_element(By.CSS_SELECTOR, ".authors")
                    authors_text = authors_element.text.strip()
                    
                    if authors_text:
                        # å°è¯•ç”¨åˆ†éš”ç¬¦åˆ†å‰²
                        authors_list = self.split_authors_text(authors_text)
                        print(f"ä»å•ä¸ªå…ƒç´ åˆ†å‰²å¾—åˆ° {len(authors_list)} ä¸ªä½œè€…")
                        
                except NoSuchElementException:
                    print("æœªæ‰¾åˆ° .authors å…ƒç´ ")
            
            # æ–¹æ³•3: å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
            if not authors_list:
                print("å°è¯•å…¶ä»–ä½œè€…é€‰æ‹©å™¨...")
                alternative_selectors = [
                    ".author",
                    ".author-name", 
                    ".author-list",
                    "span[class*='author']",
                    "div[class*='author']",
                    ".author-info",
                    ".author-detail"
                ]
                
                for selector in alternative_selectors:
                    try:
                        elements = author_area.find_elements(By.CSS_SELECTOR, selector)
                        if elements:
                            print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                            for element in elements:
                                author_text = element.text.strip()
                                if author_text and len(author_text) >= 2:
                                    authors_list.append(author_text)
                            if authors_list:
                                break
                    except Exception as e:
                        continue
            
            # æ–¹æ³•4: å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»æ•´ä¸ªauthor-areaä¸­æå–
            if not authors_list:
                print("å°è¯•ä»æ•´ä¸ªauthor-areaæ–‡æœ¬ä¸­æå–...")
                author_area_text = author_area.text.strip()
                print(f"author-areaæ–‡æœ¬: {author_area_text[:100]}...")
                
                # æŸ¥æ‰¾ä½œè€…æ¨¡å¼ï¼ˆä¸­æ–‡å§“åï¼‰
                author_patterns = [
                    r'ä½œè€…[ï¼š:]\s*([^ï¼Œ,ï¼›;\n\r]+)',
                    r'è‘—è€…[ï¼š:]\s*([^ï¼Œ,ï¼›;\n\r]+)',
                    r'ä½œè€…\s+([^ï¼Œ,ï¼›;\n\r]+)',
                    r'è‘—è€…\s+([^ï¼Œ,ï¼›;\n\r]+)'
                ]
                
                for pattern in author_patterns:
                    match = re.search(pattern, author_area_text)
                    if match:
                        authors_text = match.group(1).strip()
                        authors_list = self.split_authors_text(authors_text)
                        print(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ° {len(authors_list)} ä¸ªä½œè€…")
                        break
            
            # æ¸…ç†å’Œæ ¼å¼åŒ–ä½œè€…ä¿¡æ¯
            if authors_list:
                # å»é‡å¹¶è¿‡æ»¤
                filtered_authors = []
                for author in authors_list:
                    author = author.strip()
                    if (len(author) >= 2 and len(author) <= 20 and 
                        not re.search(r'[\d\s\-_\.\(\)\[\]{}]', author) and
                        not any(keyword in author for keyword in ['ä½œè€…', 'è‘—è€…', 'æ¥æº', 'æœŸåˆŠ', 'å‘è¡¨', 'æ—¶é—´', 'å¹´ä»½', 'æœŸ', 'å·', 'é¡µ'])):
                        filtered_authors.append(author)
                
                authors_text = ';'.join(filtered_authors)
                print(f"âœ… æœ€ç»ˆæå–åˆ° {len(filtered_authors)} ä¸ªä½œè€…: {authors_text}")
                return authors_text
            else:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•ä½œè€…ä¿¡æ¯")
                return ""
                
        except Exception as e:
            print(f"æå–ä½œè€…ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def split_authors_text(self, authors_text):
        """åˆ†å‰²ä½œè€…æ–‡æœ¬"""
        if not authors_text:
            return []
        
        # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
        separators = ['ï¼›', 'ï¼Œ', ';', ',', ' ']
        
        for separator in separators:
            if separator in authors_text:
                authors_list = [author.strip() for author in authors_text.split(separator) if author.strip()]
                if len(authors_list) > 1:
                    return authors_list
        
        # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œè¿”å›å•ä¸ªä½œè€…
        return [authors_text.strip()]
    
    def find_all_paper_elements(self):
        """æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡å…ƒç´ """
        print("ğŸ” æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡å…ƒç´ ...")
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾åˆ°æ‰€æœ‰è®ºæ–‡
        selectors_to_try = [
            ".title-area",  # ä¸»è¦é€‰æ‹©å™¨
            ".right-content .title-area",  # åœ¨right-contentä¸­
            "[class*='title-area']"  # åŒ…å«title-areaçš„ç±»
        ]
        
        all_title_areas = []
        for selector in selectors_to_try:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    print(f"é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                    all_title_areas.extend(elements)
                else:
                    print(f"é€‰æ‹©å™¨ '{selector}' æœªæ‰¾åˆ°å…ƒç´ ")
            except Exception as e:
                print(f"é€‰æ‹©å™¨ '{selector}' å‡ºé”™: {e}")
        
        # å»é‡
        unique_elements = []
        seen_elements = set()
        for element in all_title_areas:
            try:
                element_id = element.get_attribute('outerHTML')
                if element_id not in seen_elements:
                    seen_elements.add(element_id)
                    unique_elements.append(element)
            except:
                continue
        
        print(f"å»é‡åæ‰¾åˆ° {len(unique_elements)} ä¸ªå”¯ä¸€çš„ title-area")
        return unique_elements
    
    def extract_paper_from_elements(self, title_area, author_area=None):
        """ä»å…ƒç´ ä¸­æå–è®ºæ–‡ä¿¡æ¯"""
        paper = {
            'title': '',
            'authors': '',
            'authors_count': 0,
            'first_author': '',
            'all_authors': [],
            'journal': '',
            'publish_time': '',
            'link': ''
        }
        
        try:
            # æå–æ ‡é¢˜
            try:
                title_element = title_area.find_element(By.CSS_SELECTOR, ".title")
                paper['title'] = title_element.text.strip()
                
                # è·å–æ ‡é¢˜é“¾æ¥
                try:
                    title_link = title_element.find_element(By.TAG_NAME, "a")
                    if title_link:
                        paper['link'] = title_link.get_attribute('href')
                except NoSuchElementException:
                    pass
                    
            except NoSuchElementException:
                print("æœªæ‰¾åˆ° .title")
                return None
            
            # å¦‚æœæ²¡æœ‰æä¾›author_areaï¼Œå°è¯•æŸ¥æ‰¾
            if not author_area:
                author_area = self.find_author_area_for_title(title_area)
            
            if author_area:
                # ä½¿ç”¨ä¸“é—¨çš„ä½œè€…æå–å™¨
                authors_text = self.extract_authors_from_author_area(author_area)
                paper['authors'] = authors_text
                
                if authors_text:
                    authors_list = authors_text.split(';')
                    paper['authors_count'] = len(authors_list)
                    paper['first_author'] = authors_list[0] if authors_list else ''
                    paper['all_authors'] = authors_list
                else:
                    paper['authors_count'] = 0
                    paper['first_author'] = ''
                    paper['all_authors'] = []
                
                # æå–æœŸåˆŠå
                try:
                    journal_element = author_area.find_element(By.CSS_SELECTOR, ".periodical-title")
                    paper['journal'] = journal_element.text.strip()
                except NoSuchElementException:
                    print("æœªæ‰¾åˆ° .periodical-title")
                
                # æå–æ—¶é—´ - ä»æ‰€æœ‰ .authors å…ƒç´ ä¸­æŸ¥æ‰¾æ—¶é—´ä¿¡æ¯
                try:
                    # æŸ¥æ‰¾æ‰€æœ‰ .authors å…ƒç´ 
                    all_authors_elements = author_area.find_elements(By.CSS_SELECTOR, ".authors")
                    
                    for element in all_authors_elements:
                        time_text = element.text.strip()
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¶é—´ä¿¡æ¯
                        if re.search(r'(19|20)\d{2}.*æœŸ|å¹´|æœˆ|æ—¥', time_text):
                            # ä»æ—¶é—´æ–‡æœ¬ä¸­æå–å¹´ä»½
                            date_patterns = [
                                r'(19|20)\d{2}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}[æ—¥]?',
                                r'(19|20)\d{2}å¹´\d{1,2}æœŸ',
                                r'(19|20)\d{2}å¹´\d{1,2}æœˆ',
                                r'(19|20)\d{2}å¹´',
                                r'(19|20)\d{2}'
                            ]
                            
                            for pattern in date_patterns:
                                match = re.search(pattern, time_text)
                                if match:
                                    paper['publish_time'] = match.group()
                                    print(f"æ‰¾åˆ°æ—¶é—´: {paper['publish_time']}")
                                    break
                            
                            if paper['publish_time']:
                                break
                    
                    if not paper['publish_time']:
                        print("æœªæ‰¾åˆ°æ—¶é—´ä¿¡æ¯")
                            
                except Exception as e:
                    print(f"æå–æ—¶é—´ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            
            # æ¸…ç†æ•°æ®
            for key in paper:
                if isinstance(paper[key], str):
                    paper[key] = re.sub(r'\s+', ' ', paper[key]).strip()
            
            return paper if paper['title'] else None
            
        except Exception as e:
            print(f"æå–è®ºæ–‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None
    
    def find_author_area_for_title(self, title_area):
        """ä¸ºtitle-areaæŸ¥æ‰¾å¯¹åº”çš„author-area"""
        try:
            # æ–¹æ³•1: æŸ¥æ‰¾çˆ¶å…ƒç´ çš„å…„å¼Ÿå…ƒç´ 
            parent = title_area.find_element(By.XPATH, "..")
            author_area = parent.find_element(By.CSS_SELECTOR, ".author-area")
            return author_area
        except NoSuchElementException:
            try:
                # æ–¹æ³•2: æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ 
                author_area = title_area.find_element(By.XPATH, "following-sibling::*[contains(@class, 'author-area')]")
                return author_area
            except NoSuchElementException:
                try:
                    # æ–¹æ³•3: æŸ¥æ‰¾åŒ…å« title-area çš„çˆ¶å…ƒç´ ï¼Œç„¶åæŸ¥æ‰¾ author-area
                    container = title_area.find_element(By.XPATH, "ancestor::*[contains(@class, 'item') or contains(@class, 'result') or contains(@class, 'paper')]")
                    author_area = container.find_element(By.CSS_SELECTOR, ".author-area")
                    return author_area
                except NoSuchElementException:
                    return None
    
    def extract_all_papers(self):
        """æå–æ‰€æœ‰è®ºæ–‡"""
        papers = []
        
        try:
            print("ğŸ” å¼€å§‹æå–æ‰€æœ‰è®ºæ–‡...")
            
            # æŸ¥æ‰¾æ‰€æœ‰title-area
            title_areas = self.find_all_paper_elements()
            
            if not title_areas:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½• title-area")
                return []
            
            # åŒæ—¶æŸ¥æ‰¾æ‰€æœ‰author-area
            author_areas = self.driver.find_elements(By.CSS_SELECTOR, ".author-area")
            print(f"æ‰¾åˆ° {len(author_areas)} ä¸ª author-area")
            
            # æå–æ¯ç¯‡è®ºæ–‡
            for i, title_area in enumerate(title_areas):
                try:
                    print(f"\n{'='*50}")
                    print(f"å¤„ç†è®ºæ–‡ {i+1}/{len(title_areas)}")
                    
                    # å°è¯•æ‰¾åˆ°å¯¹åº”çš„author-area
                    author_area = None
                    
                    # å¦‚æœauthor-areaæ•°é‡ä¸title-areaç›¸åŒï¼ŒæŒ‰ç´¢å¼•åŒ¹é…
                    if len(author_areas) == len(title_areas):
                        author_area = author_areas[i]
                        print(f"é€šè¿‡ç´¢å¼•åŒ¹é…æ‰¾åˆ° author-area {i+1}")
                    else:
                        # å¦åˆ™å°è¯•å…¶ä»–æ–¹æ³•
                        author_area = self.find_author_area_for_title(title_area)
                        if author_area:
                            print("é€šè¿‡DOMå…³ç³»æ‰¾åˆ° author-area")
                    
                    # æå–è®ºæ–‡ä¿¡æ¯
                    paper = self.extract_paper_from_elements(title_area, author_area)
                    
                    if paper and paper['title']:
                        papers.append(paper)
                        print(f"âœ… æˆåŠŸæå–è®ºæ–‡ {len(papers)}: {paper['title'][:50]}...")
                        print(f"   ä½œè€…: {paper['authors']} (å…±{paper['authors_count']}äºº)")
                        print(f"   æœŸåˆŠ: {paper['journal']}")
                        print(f"   æ—¶é—´: {paper['publish_time']}")
                    else:
                        print(f"âŒ è®ºæ–‡ {i+1} æå–å¤±è´¥")
                    
                    # é™åˆ¶å¤„ç†æ•°é‡
                    if len(papers) >= 20:
                        print("è¾¾åˆ°æå–æ•°é‡é™åˆ¶ï¼Œåœæ­¢æå–")
                        break
                        
                except Exception as e:
                    print(f"å¤„ç†è®ºæ–‡ {i+1} æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            print(f"\nğŸ‰ æ€»å…±æˆåŠŸæå– {len(papers)} ç¯‡è®ºæ–‡")
            
            # ç»Ÿè®¡ä½œè€…ä¿¡æ¯
            total_authors = sum(paper['authors_count'] for paper in papers)
            multi_author_papers = sum(1 for paper in papers if paper['authors_count'] > 1)
            
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   æ€»è®ºæ–‡æ•°: {len(papers)}")
            print(f"   æ€»ä½œè€…æ•°: {total_authors}")
            print(f"   å¤šä½œè€…è®ºæ–‡: {multi_author_papers} ç¯‡")
            print(f"   å•ä½œè€…è®ºæ–‡: {len(papers) - multi_author_papers} ç¯‡")
            
            return papers
            
        except Exception as e:
            print(f"æå–æ‰€æœ‰è®ºæ–‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def scrape_first_page(self, url):
        """æŠ“å–ç¬¬ä¸€é¡µè®ºæ–‡æ•°æ®"""
        if not self.setup_driver():
            return []
        
        try:
            print(f"æ­£åœ¨è®¿é—®: {url}")
            self.driver.get(url)
            
            if not self.wait_for_page_load():
                print("é¡µé¢åŠ è½½å¤±è´¥")
                return []
            
            # æå–æ‰€æœ‰è®ºæ–‡
            papers = self.extract_all_papers()
            
            self.papers_data = papers
            return papers
            
        except Exception as e:
            print(f"æŠ“å–è¿‡ç¨‹å¤±è´¥: {e}")
            return []
        
        finally:
            if self.driver:
                self.driver.quit()
    
    def save_to_excel(self, papers, filename=None):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶"""
        if filename is None:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ä¸‡æ–¹æ•°æ®è®ºæ–‡_ç‹¬ç«‹ä½œè€…_{timestamp}.xlsx"
        
        if not papers:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            data = []
            for paper in papers:
                data.append({
                    'ä½œè€…(arrayAuthor)': paper.get('authors', ''),
                    'ä½œè€…æ•°é‡': paper.get('authors_count', 0),
                    'ç¬¬ä¸€ä½œè€…': paper.get('first_author', ''),
                    'æ‰€æœ‰ä½œè€…': ';'.join(paper.get('all_authors', [])),
                    'è®ºæ–‡å(arrayTitle)': paper.get('title', ''),
                    'æœŸåˆŠå(arrayJournal)': paper.get('journal', ''),
                    'å‘è¡¨æ—¶é—´(arrayTime)': paper.get('publish_time', ''),
                    'é“¾æ¥(arrayHref)': paper.get('link', '')
                })
            
            df = pd.DataFrame(data)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
            
        except Exception as e:
            print(f"ä¿å­˜å¤±è´¥: {e}")
            return False

def main():
    print("ğŸš€ ä¸‡æ–¹æ•°æ®ç‹¬ç«‹ä½œè€…å…ƒç´ çˆ¬è™«")
    print("=" * 50)
    print("ä¸“é—¨å¤„ç†æ¯ä¸ªä½œè€…éƒ½æ˜¯ç‹¬ç«‹spanå…ƒç´ çš„æƒ…å†µ")
    print("ç»“æ„: <span class='authors'>å¼ é–</span>")
    print("      <span class='authors'>é™†æ€æ™“</span>")
    print("      <span class='authors'>æ–¹å®¶å¿ </span>")
    print("=" * 50)
    
    url = "https://s.wanfangdata.com.cn/paper?q=%E4%BD%9C%E8%80%85%E5%8D%95%E4%BD%8D%3A%E9%A6%96%E9%83%BD%E5%9B%BE%E4%B9%A6%E9%A6%86&p=1"

    try:
        print("ğŸ“± æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        scraper = IndependentAuthorWanfangScraper()
        print("ğŸ” å¼€å§‹æŠ“å–è®ºæ–‡æ•°æ®...")
        papers = scraper.scrape_first_page(url)

        if papers:
            print(f"âœ… æˆåŠŸæŠ“å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            
            # æ˜¾ç¤ºè®ºæ–‡ä¿¡æ¯
            print("\nğŸ“‹ æŠ“å–åˆ°çš„è®ºæ–‡:")
            for i, paper in enumerate(papers, 1):
                print(f"\nç¬¬ {i} ç¯‡:")
                print(f"æ ‡é¢˜: {paper.get('title', 'N/A')}")
                print(f"ä½œè€…: {paper.get('authors', 'N/A')} (å…±{paper.get('authors_count', 0)}äºº)")
                print(f"ç¬¬ä¸€ä½œè€…: {paper.get('first_author', 'N/A')}")
                print(f"æœŸåˆŠ: {paper.get('journal', 'N/A')}")
                print(f"æ—¶é—´: {paper.get('publish_time', 'N/A')}")
                print(f"é“¾æ¥: {paper.get('link', 'N/A')[:50]}...")
            
            print("\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®...")
            scraper.save_to_excel(papers)
            print("ğŸ‰ æŠ“å–å®Œæˆï¼")
        else:
            print("âŒ æ²¡æœ‰æŠ“å–åˆ°æ•°æ®")

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
