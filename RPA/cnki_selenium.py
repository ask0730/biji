import time
import pandas as pd
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class CNKISeleniumScraper:
    def __init__(self):
        self.driver = None
        self.papers_data = []

    def is_valid_journal_name(self, journal_name):
        """éªŒè¯æœŸåˆŠåæ˜¯å¦æœ‰æ•ˆ"""
        if not journal_name or len(journal_name) < 2:
            return False

        # æ’é™¤æ˜æ˜¾é”™è¯¯çš„æœŸåˆŠå
        invalid_patterns = [
            'HTML', 'CSS', 'JavaScript', 'PDF', 'DOC', 'DOCX', 'XLS', 'XLSX',
            'http', 'www', '.com', '.cn', '.org', '.net',
            'ä¸‹è½½', 'æŸ¥çœ‹', 'è¯¦æƒ…', 'å…¨æ–‡', 'æ‘˜è¦', 'é“¾æ¥', 'ç‚¹å‡»',
            'ä½œè€…', 'å‘è¡¨', 'å‡ºç‰ˆ', 'ç¼–è¾‘', 'å®¡ç¨¿',
            'ç¬¬ä¸€é¡µ', 'ç¬¬äºŒé¡µ', 'ä¸Šä¸€é¡µ', 'ä¸‹ä¸€é¡µ',
            'æœç´¢', 'æ£€ç´¢', 'ç­›é€‰', 'æ’åº',
            'é¦–éƒ½å›¾ä¹¦é¦†', 'å…¬å…±å›¾ä¹¦é¦†', 'æœåŠ¡', 'ç®¡ç†', 'å·¥ä½œ', 'åˆ†æ', 'æ¢è®¨', 'åº”ç”¨', 'å®è·µ', 'æ¢ç´¢',
            'æŒ‘æˆ˜', 'å¯¹ç­–', 'èƒŒæ™¯', 'èåˆ', 'æŠ€æœ¯', 'ä¼ æ‰¿', 'æ™ºæ…§', 'æ—…æ¸¸'
        ]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ— æ•ˆæ¨¡å¼
        for pattern in invalid_patterns:
            if pattern.lower() in journal_name.lower():
                return False

        # æœŸåˆŠåä¸åº”è¯¥å…¨æ˜¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
        if re.match(r'^[\d\s\-_\.]+$', journal_name):
            return False

        # æœŸåˆŠååº”è¯¥åŒ…å«ä¸­æ–‡å­—ç¬¦
        if not re.search(r'[\u4e00-\u9fa5]', journal_name):
            return False

        # æœŸåˆŠåé•¿åº¦åº”è¯¥åˆç†ï¼ˆæ ¹æ®å›¾ç‰‡ä¸­çš„æœŸåˆŠåï¼Œé€šå¸¸2-10ä¸ªå­—ç¬¦ï¼‰
        if len(journal_name) > 15 or len(journal_name) < 2:
            return False

        # æœŸåˆŠåä¸åº”è¯¥åŒ…å«æ ‡ç‚¹ç¬¦å·ï¼ˆé™¤äº†å¸¸è§çš„æœŸåˆŠç¬¦å·ï¼‰
        if re.search(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘\(\)]+', journal_name):
            return False

        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæœŸåˆŠåçš„å¸¸è§æ¨¡å¼
        valid_patterns = [
            r'.*é˜…è¯».*', r'.*å­¦åˆŠ.*', r'.*å­¦æŠ¥.*', r'.*ç ”ç©¶.*', r'.*æ‚å¿—.*',
            r'.*æœŸåˆŠ.*', r'.*é€šè®¯.*', r'.*æ–‡æ‘˜.*', r'.*è¯„è®º.*', r'.*å¹´é‰´.*',
            r'.*å‚è€ƒ.*', r'.*æ–°ä¹¦.*', r'.*æ–‡åŒ–.*', r'.*äº§ä¸š.*', r'.*ç®¡ç†.*',
            r'.*ç§‘æŠ€.*', r'.*å­¦æœ¯.*', r'.*æ•™è‚².*', r'.*ä¿¡æ¯.*'
        ]

        # å¦‚æœåŒ…å«æœŸåˆŠå…³é”®è¯ï¼Œæ›´å¯èƒ½æ˜¯æœ‰æ•ˆæœŸåˆŠå
        for pattern in valid_patterns:
            if re.match(pattern, journal_name):
                return True

        # å¯¹äºç®€çŸ­çš„ä¸­æ–‡è¯æ±‡ï¼ˆå¦‚"å‚è€ƒ"ã€"å¹´é‰´ç¼–"ï¼‰ï¼Œä¹Ÿå¯èƒ½æ˜¯æœŸåˆŠå
        if len(journal_name) <= 6 and re.match(r'^[\u4e00-\u9fa5]+$', journal_name):
            return True

        return False
        
    def setup_driver(self):
        try:
            chrome_options = Options()
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(10)
            return True
            
        except Exception as e:
            print(f"æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def wait_for_results(self, timeout=30):
        try:
            wait = WebDriverWait(self.driver, timeout)
            selectors = [
                "#gridTable",
                ".gridTable",
                "#briefBox",
                ".result-table-list",
                "table[class*='result']",
                "tr[class*='result']"
            ]
            
            for selector in selectors:
                try:
                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                    return True
                except TimeoutException:
                    continue
            
            time.sleep(5)
            tables = self.driver.find_elements(By.TAG_NAME, "table")
            if tables:
                return True
            
            links = self.driver.find_elements(By.TAG_NAME, "a")
            paper_links = [link for link in links if link.text and len(link.text) > 10]
            if paper_links:
                return True
            
            return False
            
        except Exception as e:
            print(f"ç­‰å¾…ç»“æœå¤±è´¥: {e}")
            return False
    
    def extract_papers_from_page(self):
        papers = []
        
        try:
            tables = self.driver.find_elements(By.TAG_NAME, "table")
            for table in tables:
                rows = table.find_elements(By.TAG_NAME, "tr")
                for row in rows:
                    paper = self.extract_paper_from_row(row)
                    if paper and paper['title']:
                        papers.append(paper)
            
            if not papers:
                links = self.driver.find_elements(By.TAG_NAME, "a")
                for link in links:
                    try:
                        href = link.get_attribute('href')
                        text = link.text.strip()
                        
                        if (text and len(text) > 10 and 
                            href and ('detail' in href or 'kcms' in href)):
                            
                            paper = {
                                'title': text,
                                'authors': '',
                                'journal': '',
                                'publish_time': '',
                                'link': href
                            }
                            
                            parent = link.find_element(By.XPATH, "..")
                            parent_text = parent.text

                            # ä¼˜å…ˆåŒ¹é…å®Œæ•´æ—¥æœŸæ ¼å¼ YYYY-MM-DD
                            date_match = re.search(r'(19|20)\d{2}-\d{1,2}-\d{1,2}', parent_text)
                            if date_match:
                                paper['publish_time'] = date_match.group()
                            else:
                                # åŒ¹é…å¹´ä»½
                                year_match = re.search(r'(19|20)\d{2}', parent_text)
                                if year_match:
                                    paper['publish_time'] = year_match.group()
                            
                            papers.append(paper)
                            
                    except Exception:
                        continue
            
            return papers
            
        except Exception as e:
            print(f"æå–è®ºæ–‡ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def extract_paper_from_row(self, row):
        paper = {
            'title': '',
            'authors': '',
            'journal': '',
            'publish_time': '',
            'link': ''
        }

        try:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            links = row.find_elements(By.TAG_NAME, "a")
            for link in links:
                href = link.get_attribute('href')
                text = link.text.strip()

                if text and len(text) > 10 and href:
                    paper['title'] = text
                    paper['link'] = href
                    break

            if not paper['title']:
                return None

            # è·å–æ‰€æœ‰å•å…ƒæ ¼
            cells = row.find_elements(By.TAG_NAME, "td")
            row_text = row.text

            # åˆ†ææ•´è¡Œæ–‡æœ¬ï¼Œç”¨äºæ›´å¥½çš„å­—æ®µè¯†åˆ«
            row_text = row.text.strip()

            # æ›´ç²¾ç¡®çš„å­—æ®µæå– - åŸºäºCNKIå®é™…é¡µé¢ç»“æ„
            for i, cell in enumerate(cells):
                cell_text = cell.text.strip()
                if not cell_text:
                    continue

                # æå–ä½œè€… - CNKIä¸­ä½œè€…é€šå¸¸åœ¨ç‰¹å®šä½ç½®
                if not paper['authors']:
                    # æ–¹æ³•1: æŸ¥æ‰¾æ˜ç¡®æ ‡æ³¨ä½œè€…çš„å•å…ƒæ ¼
                    if 'ä½œè€…' in cell_text and ('ä½œè€…:' in cell_text or 'ä½œè€…ï¼š' in cell_text):
                        authors = cell_text.replace('ä½œè€…:', '').replace('ä½œè€…ï¼š', '').strip()
                        if authors and not re.search(r'\d{4}', authors) and len(authors) < 100:
                            paper['authors'] = authors
                    # æ–¹æ³•2: æŸ¥æ‰¾ä½œè€…æ¨¡å¼ï¼ˆæ’é™¤æ ‡é¢˜ï¼‰
                    elif (i >= 1 and cell_text != paper['title'] and  # ä¸æ˜¯æ ‡é¢˜
                          not re.search(r'\d{4}', cell_text) and
                          not any(keyword in cell_text for keyword in ['æ¥æº', 'æœŸåˆŠ', 'å‘è¡¨', 'å‡ºç‰ˆ', 'å¹´', 'æœˆ', 'æ—¥', 'å­¦æŠ¥', 'ç ”ç©¶', 'æ‚å¿—', 'æŠ€æœ¯', 'ç®¡ç†', 'æ•™è‚²', 'æ–‡åŒ–', 'ç¤¾ä¼š', 'ç»æµ', 'æ³•å­¦', 'åŒ»å­¦', 'å·¥ç¨‹', 'ä¿¡æ¯', 'ç°ä»£', 'å½“ä»£', 'ä¸­å›½', 'å›½é™…', 'å¤§å­¦', 'å­¦é™¢']) and
                          len(cell_text) < 80 and len(cell_text) > 1 and  # åˆç†çš„ä½œè€…åé•¿åº¦
                          re.search(r'[\u4e00-\u9fa5]', cell_text)):  # åŒ…å«ä¸­æ–‡å­—ç¬¦
                        # è¿›ä¸€æ­¥éªŒè¯æ˜¯å¦åƒä½œè€…åï¼ˆé€šå¸¸è¾ƒçŸ­ï¼Œå¯èƒ½æœ‰åˆ†å·ï¼‰
                        if (';' in cell_text or 'ï¼›' in cell_text or  # å¤šä½œè€…åˆ†éš”ç¬¦
                            (len(cell_text) >= 2 and len(cell_text) <= 30 and
                             not any(char in cell_text for char in ['ï¼ˆ', 'ï¼‰', '(', ')', 'ã€', 'ã€‘', '[', ']']))):  # ä¸åŒ…å«æ‹¬å·ç­‰
                            paper['authors'] = cell_text

                # æå–æœŸåˆŠåï¼ˆæ¥æºï¼‰- CNKIä¸­æœŸåˆŠä¿¡æ¯çš„è¯†åˆ«
                if not paper['journal']:
                    # æ–¹æ³•1: æŸ¥æ‰¾æ˜ç¡®æ ‡æ³¨æ¥æºçš„å•å…ƒæ ¼
                    if 'æ¥æº' in cell_text:
                        # æ›´ç²¾ç¡®çš„æ¥æºæå–
                        if 'æ¥æº:' in cell_text or 'æ¥æºï¼š' in cell_text:
                            # æå–æ¥æºåé¢çš„å†…å®¹
                            journal_match = re.search(r'æ¥æº[ï¼š:]\s*([^ï¼Œ,ï¼›;\s]+)', cell_text)
                            if journal_match:
                                journal = journal_match.group(1).strip()
                            else:
                                journal = cell_text.replace('æ¥æº:', '').replace('æ¥æºï¼š', '').strip()
                        else:
                            # å¦‚æœåªæ˜¯åŒ…å«"æ¥æº"ä½†æ²¡æœ‰å†’å·ï¼Œå¯èƒ½æ˜¯æœŸåˆŠåçš„ä¸€éƒ¨åˆ†
                            journal = cell_text.strip()

                        # ç§»é™¤æ—¥æœŸéƒ¨åˆ†
                        journal = re.sub(r'\s*(19|20)\d{2}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}[æ—¥]?.*', '', journal)
                        journal = re.sub(r'\s*(19|20)\d{2}.*', '', journal).strip()

                        # éªŒè¯æœŸåˆŠåçš„æœ‰æ•ˆæ€§
                        if journal and len(journal) > 1 and self.is_valid_journal_name(journal):
                            paper['journal'] = journal

                    # æ–¹æ³•2: æŸ¥æ‰¾æœŸåˆŠæ¨¡å¼ - åŸºäºå›¾ç‰‡ä¸­çš„æœŸåˆŠåç‰¹å¾
                    elif (cell_text != paper['title'] and  # ä¸æ˜¯æ ‡é¢˜
                          cell_text != paper['authors'] and  # ä¸æ˜¯ä½œè€…
                          not re.search(r'\d{4}', cell_text) and  # ä¸åŒ…å«å¹´ä»½
                          len(cell_text) <= 15 and len(cell_text) >= 2 and  # é•¿åº¦åˆç†
                          re.search(r'[\u4e00-\u9fa5]', cell_text) and  # åŒ…å«ä¸­æ–‡
                          self.is_valid_journal_name(cell_text)):
                        paper['journal'] = cell_text

                    # æ–¹æ³•3: ä½ç½®æ¨æ–­ï¼ˆé€šå¸¸æœŸåˆŠååœ¨ç¬¬2-4åˆ—ï¼‰
                    elif (i >= 1 and i <= 4 and not paper['journal'] and
                          cell_text != paper['title'] and  # ä¸æ˜¯æ ‡é¢˜
                          cell_text != paper['authors'] and  # ä¸æ˜¯ä½œè€…
                          not re.search(r'\d{4}', cell_text) and
                          len(cell_text) <= 12 and len(cell_text) >= 2 and  # æœŸåˆŠåé€šå¸¸è¾ƒçŸ­
                          re.match(r'^[\u4e00-\u9fa5]+$', cell_text) and  # çº¯ä¸­æ–‡
                          self.is_valid_journal_name(cell_text)):
                        paper['journal'] = cell_text

                # æå–å‘è¡¨æ—¶é—´ - æŸ¥æ‰¾æ—¥æœŸæ ¼å¼
                if not paper['publish_time'] and re.search(r'(19|20)\d{2}', cell_text):
                    # ä¼˜å…ˆåŒ¹é…å®Œæ•´æ—¥æœŸæ ¼å¼ YYYY-MM-DD
                    date_match = re.search(r'(19|20)\d{2}-\d{1,2}-\d{1,2}', cell_text)
                    if date_match:
                        paper['publish_time'] = date_match.group()
                    else:
                        # åŒ¹é…å¹´ä»½
                        year_match = re.search(r'(19|20)\d{2}', cell_text)
                        if year_match:
                            paper['publish_time'] = year_match.group()

            # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°å‘è¡¨æ—¶é—´ï¼Œä»æ•´è¡Œæ–‡æœ¬ä¸­æå–
            if not paper['publish_time']:
                # ä¼˜å…ˆåŒ¹é…å®Œæ•´æ—¥æœŸæ ¼å¼
                date_match = re.search(r'(19|20)\d{2}-\d{1,2}-\d{1,2}', row_text)
                if date_match:
                    paper['publish_time'] = date_match.group()
                else:
                    # åŒ¹é…å¹´ä»½
                    year_match = re.search(r'(19|20)\d{2}', row_text)
                    if year_match:
                        paper['publish_time'] = year_match.group()

            # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°æœŸåˆŠåï¼Œå°è¯•ä»æ•´è¡Œæ–‡æœ¬ä¸­æå–
            if not paper['journal'] or paper['journal'] == 'æœŸåˆŠ':
                # æ–¹æ³•1: ä»æ•´è¡Œæ–‡æœ¬ä¸­æŸ¥æ‰¾"æ¥æº:"æ¨¡å¼
                source_match = re.search(r'æ¥æº[ï¼š:]\s*([^ï¼Œ,ï¼›;\s\d]+)', row_text)
                if source_match:
                    journal_candidate = source_match.group(1).strip()
                    if self.is_valid_journal_name(journal_candidate):
                        paper['journal'] = journal_candidate

                # æ–¹æ³•2: å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾æœŸåˆŠå…³é”®è¯
                if not paper['journal'] or paper['journal'] == 'æœŸåˆŠ':
                    row_parts = row_text.split()
                    for part in row_parts:
                        if (part != paper['title'] and part != paper['authors'] and
                            len(part) > 3 and len(part) < 30 and
                            not re.search(r'\d{4}', part) and
                            re.search(r'[\u4e00-\u9fa5]', part) and
                            any(keyword in part for keyword in ['å­¦æŠ¥', 'ç ”ç©¶', 'æ‚å¿—', 'æœŸåˆŠ', 'å­¦åˆŠ', 'é€šè®¯', 'æ–‡æ‘˜', 'è¯„è®º', 'å­¦æœ¯', 'ç§‘å­¦', 'æŠ€æœ¯', 'ç®¡ç†', 'æ•™è‚²', 'æ–‡åŒ–', 'ç¤¾ä¼š', 'ç»æµ', 'æ³•å­¦', 'åŒ»å­¦', 'å·¥ç¨‹', 'ä¿¡æ¯', 'ç°ä»£', 'å½“ä»£', 'ä¸­å›½', 'å›½é™…', 'å¤§å­¦', 'å­¦é™¢', 'å›¾ä¹¦', 'é˜…è¯»', 'çŸ¥è¯†', 'æ™ºæ…§', 'æ•°å­—', 'ç½‘ç»œ']) and
                            self.is_valid_journal_name(part)):
                            paper['journal'] = part
                            break

                # æ–¹æ³•3: æœ€åå°è¯•æ›´å®½æ¾çš„åŒ¹é…
                if not paper['journal'] or paper['journal'] == 'æœŸåˆŠ':
                    # æŸ¥æ‰¾å¯èƒ½çš„æœŸåˆŠåæ¨¡å¼ï¼ˆä¸­æ–‡è¯æ±‡ï¼Œä¸åŒ…å«å¸¸è§åŠ¨è¯ï¼‰
                    journal_candidates = re.findall(r'[\u4e00-\u9fa5]{2,15}', row_text)
                    for candidate in journal_candidates:
                        if (candidate != paper['title'] and candidate != paper['authors'] and
                            candidate not in ['é¦–éƒ½å›¾ä¹¦é¦†', 'å…¬å…±å›¾ä¹¦é¦†', 'å›¾ä¹¦é¦†', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'è¿›è¡Œ', 'å®ç°', 'æé«˜', 'å‘å±•', 'å»ºè®¾', 'å®Œå–„', 'åŠ å¼º', 'æœåŠ¡', 'ç®¡ç†', 'å·¥ä½œ', 'ç ”ç©¶', 'åˆ†æ', 'æ¢è®¨', 'åº”ç”¨', 'å®è·µ', 'æ¢ç´¢'] and
                            self.is_valid_journal_name(candidate)):
                            paper['journal'] = candidate
                            break

            # æ¸…ç†æ•°æ®
            for key in paper:
                if isinstance(paper[key], str):
                    paper[key] = re.sub(r'\s+', ' ', paper[key]).strip()

            return paper

        except Exception as e:
            return None
    
    def scrape_papers(self, url, max_pages=3):
        if not self.setup_driver():
            return []
        
        try:
            self.driver.get(url)
            time.sleep(3)
            
            all_papers = []
            
            for page in range(1, max_pages + 1):
                if page > 1:
                    try:
                        next_button = self.driver.find_element(By.CSS_SELECTOR, ".but-r, .next, [title='ä¸‹ä¸€é¡µ']")
                        if next_button.is_enabled():
                            next_button.click()
                            time.sleep(3)
                        else:
                            break
                    except NoSuchElementException:
                        break
                
                if self.wait_for_results():
                    papers = self.extract_papers_from_page()
                    if papers:
                        all_papers.extend(papers)
                    else:
                        if page > 1:
                            break
                else:
                    break
                
                if page < max_pages:
                    time.sleep(2)
            
            self.papers_data = all_papers
            return all_papers
            
        except Exception as e:
            print(f"æŠ“å–è¿‡ç¨‹å¤±è´¥: {e}")
            return []
        
        finally:
            if self.driver:
                self.driver.quit()
    
    def save_to_excel(self, papers, filename=None):
        if filename is None:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"é¦–éƒ½å›¾ä¹¦é¦†ç›¸å…³è®ºæ–‡_{timestamp}.xlsx"
        if not papers:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            data = []
            for paper in papers:
                data.append({
                    'ä½œè€…(arrayAuthor)': paper.get('authors', ''),
                    'è®ºæ–‡å(arrayTitle)': paper.get('title', ''),
                    'æœŸåˆŠå(arrayJournal)': paper.get('journal', ''),
                    'å‘è¡¨æ—¶é—´(arrayTime)': paper.get('publish_time', ''),
                    'é“¾æ¥(arrayHref)': paper.get('link', '')
                })
            
            df = pd.DataFrame(data)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
            
        except Exception as e:
            print(f"ä¿å­˜å¤±è´¥: {e}")
            return False

def main():
    print("ğŸš€ å¼€å§‹è¿è¡ŒCNKIè®ºæ–‡æŠ“å–å·¥å…·...")
    url = "https://kns.cnki.net/kns8s/defaultresult/index?crossids=YSTT4HG0%2CLSTPFY1C%2CJUP3MUPD%2CMPMFIG1A%2CWQ0UVIAA%2CBLZOG7CK%2CPWFIRAGL%2CEMRPGLPA%2CNLBO1Z6R%2CNN3FJMUV&korder=AF&kw=%E9%A6%96%E9%83%BD%E5%9B%BE%E4%B9%A6%E9%A6%86"

    try:
        print("ğŸ“± æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        scraper = CNKISeleniumScraper()
        print("ğŸ” å¼€å§‹æŠ“å–è®ºæ–‡æ•°æ®...")
        papers = scraper.scrape_papers(url, max_pages=1)

        if papers:
            print(f"âœ… æˆåŠŸæŠ“å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°Excelæ–‡ä»¶...")
            scraper.save_to_excel(papers)
            print("ğŸ‰ æŠ“å–å®Œæˆï¼")
        else:
            print("âŒ æ²¡æœ‰æŠ“å–åˆ°æ•°æ®")

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
