#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CNKIè®ºæ–‡ä¿¡æ¯æŠ“å–å·¥å…· - Seleniumç‰ˆæœ¬
æŠ“å–é¦–éƒ½å›¾ä¹¦é¦†ç›¸å…³è®ºæ–‡ä¿¡æ¯ï¼ˆä»…ç¬¬ä¸€é¡µï¼‰
"""

import time
import pandas as pd
import re
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class CNKISeleniumScraper:
    def __init__(self):
        self.driver = None
        self.papers_data = []
        
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            # ä¸ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼Œè¿™æ ·å¯ä»¥çœ‹åˆ°æµè§ˆå™¨æ“ä½œ
            # chrome_options.add_argument('--headless')
            
            # è®¾ç½®ç”¨æˆ·ä»£ç†
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(10)
            
            print("âœ… Chromeæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨å’ŒChromeDriver")
            return False
    
    def wait_for_results(self, timeout=30):
        """ç­‰å¾…æœç´¢ç»“æœåŠ è½½"""
        try:
            print("â³ ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")
            
            # ç­‰å¾…ç»“æœå®¹å™¨å‡ºç°
            wait = WebDriverWait(self.driver, timeout)
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„é€‰æ‹©å™¨
            selectors = [
                "#gridTable",
                ".gridTable",
                "#briefBox",
                ".result-table-list",
                "table[class*='result']",
                "tr[class*='result']"
            ]
            
            for selector in selectors:
                try:
                    element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                    print(f"âœ… æ‰¾åˆ°ç»“æœå®¹å™¨: {selector}")
                    return True
                except TimeoutException:
                    continue
            
            print("âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†ç»“æœå®¹å™¨ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–å…ƒç´ ...")
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            time.sleep(5)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è¡¨æ ¼æˆ–åˆ—è¡¨
            tables = self.driver.find_elements(By.TAG_NAME, "table")
            if tables:
                print(f"âœ… æ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
                return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é“¾æ¥
            links = self.driver.find_elements(By.TAG_NAME, "a")
            paper_links = [link for link in links if link.text and len(link.text) > 10]
            if paper_links:
                print(f"âœ… æ‰¾åˆ° {len(paper_links)} ä¸ªå¯èƒ½çš„è®ºæ–‡é“¾æ¥")
                return True
            
            return False
            
        except Exception as e:
            print(f"âŒ ç­‰å¾…ç»“æœå¤±è´¥: {e}")
            return False
    
    def extract_papers_from_page(self):
        """ä»å½“å‰é¡µé¢æå–è®ºæ–‡ä¿¡æ¯"""
        papers = []
        
        try:
            # æ–¹æ³•1: å°è¯•ä»è¡¨æ ¼ä¸­æå–
            tables = self.driver.find_elements(By.TAG_NAME, "table")
            for table in tables:
                rows = table.find_elements(By.TAG_NAME, "tr")
                for row in rows:
                    paper = self.extract_paper_from_row(row)
                    if paper and paper['title']:
                        papers.append(paper)
            
            # æ–¹æ³•2: å¦‚æœè¡¨æ ¼æ–¹æ³•æ²¡æœ‰ç»“æœï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            if not papers:
                links = self.driver.find_elements(By.TAG_NAME, "a")
                for link in links:
                    try:
                        href = link.get_attribute('href')
                        text = link.text.strip()
                        
                        if (text and len(text) > 10 and 
                            href and ('detail' in href or 'kcms' in href)):
                            
                            paper = {
                                'title': text,
                                'authors': '',
                                'journal': '',
                                'publish_time': '',
                                'link': href
                            }
                            
                            # å°è¯•ä»çˆ¶å…ƒç´ è·å–æ›´å¤šä¿¡æ¯
                            parent = link.find_element(By.XPATH, "..")
                            parent_text = parent.text
                            
                            # æå–å¹´ä»½
                            year_match = re.search(r'(19|20)\d{2}', parent_text)
                            if year_match:
                                paper['publish_time'] = year_match.group()
                            
                            papers.append(paper)
                            
                    except Exception:
                        continue
            
            print(f"ğŸ“„ å½“å‰é¡µé¢æå–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            print(f"âŒ æå–è®ºæ–‡ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def extract_paper_from_row(self, row):
        """ä»è¡¨æ ¼è¡Œæå–è®ºæ–‡ä¿¡æ¯"""
        paper = {
            'title': '',
            'authors': '',
            'journal': '',
            'publish_time': '',
            'link': ''
        }
        
        try:
            # æŸ¥æ‰¾æ ‡é¢˜é“¾æ¥
            links = row.find_elements(By.TAG_NAME, "a")
            for link in links:
                href = link.get_attribute('href')
                text = link.text.strip()
                
                if text and len(text) > 10 and href:
                    paper['title'] = text
                    paper['link'] = href
                    break
            
            if not paper['title']:
                return None
            
            # è·å–è¡Œçš„æ‰€æœ‰å•å…ƒæ ¼
            cells = row.find_elements(By.TAG_NAME, "td")
            row_text = row.text
            
            # å°è¯•æå–ä½œè€…ã€æœŸåˆŠã€æ—¶é—´ç­‰ä¿¡æ¯
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„é¡µé¢ç»“æ„è°ƒæ•´
            for i, cell in enumerate(cells):
                cell_text = cell.text.strip()
                if not cell_text:
                    continue
                
                # ç®€å•çš„å¯å‘å¼è§„åˆ™
                if i == 1 or 'ä½œè€…' in cell_text:
                    paper['authors'] = cell_text.replace('ä½œè€…:', '').strip()
                elif i == 2 or 'æœŸåˆŠ' in cell_text or 'æ¥æº' in cell_text:
                    paper['journal'] = cell_text.replace('æœŸåˆŠ:', '').replace('æ¥æº:', '').strip()
                elif re.search(r'(19|20)\d{2}', cell_text):
                    year_match = re.search(r'(19|20)\d{2}', cell_text)
                    if year_match:
                        paper['publish_time'] = year_match.group()
            
            # å¦‚æœæ²¡æœ‰ä»å•å…ƒæ ¼è·å–åˆ°ä¿¡æ¯ï¼Œå°è¯•ä»æ•´è¡Œæ–‡æœ¬æå–
            if not paper['publish_time']:
                year_match = re.search(r'(19|20)\d{2}', row_text)
                if year_match:
                    paper['publish_time'] = year_match.group()
            
            return paper
            
        except Exception as e:
            return None
    
    def scrape_papers(self, url, max_pages=3):
        """æŠ“å–è®ºæ–‡ä¿¡æ¯"""
        if not self.setup_driver():
            return []
        
        try:
            print(f"ğŸ¯ å¼€å§‹è®¿é—®: {url}")
            self.driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            
            all_papers = []
            
            for page in range(1, max_pages + 1):
                print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µ...")
                
                if page > 1:
                    # å°è¯•ç‚¹å‡»ä¸‹ä¸€é¡µ
                    try:
                        next_button = self.driver.find_element(By.CSS_SELECTOR, ".but-r, .next, [title='ä¸‹ä¸€é¡µ']")
                        if next_button.is_enabled():
                            next_button.click()
                            time.sleep(3)
                        else:
                            print("âŒ æ²¡æœ‰æ›´å¤šé¡µé¢")
                            break
                    except NoSuchElementException:
                        print("âŒ æ‰¾ä¸åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
                        break
                
                # ç­‰å¾…ç»“æœåŠ è½½
                if self.wait_for_results():
                    papers = self.extract_papers_from_page()
                    if papers:
                        all_papers.extend(papers)
                        print(f"âœ… ç¬¬ {page} é¡µæˆåŠŸæå– {len(papers)} ç¯‡è®ºæ–‡")
                    else:
                        print(f"âš ï¸ ç¬¬ {page} é¡µæ²¡æœ‰æå–åˆ°è®ºæ–‡")
                        if page > 1:
                            break
                else:
                    print(f"âŒ ç¬¬ {page} é¡µåŠ è½½å¤±è´¥")
                    break
                
                # é¡µé¢é—´å»¶è¿Ÿ
                if page < max_pages:
                    time.sleep(2)
            
            self.papers_data = all_papers
            print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼å…±è·å– {len(all_papers)} ç¯‡è®ºæ–‡")
            return all_papers
            
        except Exception as e:
            print(f"âŒ æŠ“å–è¿‡ç¨‹å¤±è´¥: {e}")
            return []
        
        finally:
            if self.driver:
                self.driver.quit()
                print("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
    
    def save_to_excel(self, papers, filename="é¦–éƒ½å›¾ä¹¦é¦†ç›¸å…³è®ºæ–‡.xlsx"):
        """ä¿å­˜åˆ°Excel"""
        if not papers:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            data = []
            for paper in papers:
                data.append({
                    'ä½œè€…(arrayAuthor)': paper.get('authors', ''),
                    'è®ºæ–‡å(arrayTitle)': paper.get('title', ''),
                    'æœŸåˆŠå(arrayJournal)': paper.get('journal', ''),
                    'å‘è¡¨æ—¶é—´(arrayTime)': paper.get('publish_time', ''),
                    'é“¾æ¥(arrayHref)': paper.get('link', '')
                })
            
            df = pd.DataFrame(data)
            df.to_excel(filename, index=False, engine='openpyxl')
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            print(f"ğŸ“Š å…±ä¿å­˜ {len(data)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    url = "https://kns.cnki.net/kns8s/defaultresult/index?crossids=YSTT4HG0%2CLSTPFY1C%2CJUP3MUPD%2CMPMFIG1A%2CWQ0UVIAA%2CBLZOG7CK%2CPWFIRAGL%2CEMRPGLPA%2CNLBO1Z6R%2CNN3FJMUV&korder=AF&kw=%E9%A6%96%E9%83%BD%E5%9B%BE%E4%B9%A6%E9%A6%86"
    
    print("ğŸš€ CNKIè®ºæ–‡ä¿¡æ¯æŠ“å–å·¥å…· - Seleniumç‰ˆæœ¬")
    print("=" * 60)
    print(f"ğŸ” æœç´¢å…³é”®è¯: é¦–éƒ½å›¾ä¹¦é¦†")
    print(f"ğŸŒ ç›®æ ‡ç½‘ç«™: CNKIä¸­å›½çŸ¥ç½‘")
    print("=" * 60)
    
    try:
        scraper = CNKISeleniumScraper()
        papers = scraper.scrape_papers(url, max_pages=1)
        
        if papers:
            # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®
            print(f"\nğŸ“‹ æ•°æ®é¢„è§ˆ (å‰3æ¡):")
            for i, paper in enumerate(papers[:3], 1):
                print(f"\n{i}. ğŸ“„ {paper['title'][:60]}...")
                print(f"   ğŸ‘¤ ä½œè€…: {paper['authors'][:40]}...")
                print(f"   ğŸ“– æœŸåˆŠ: {paper['journal'][:40]}...")
                print(f"   ğŸ“… æ—¶é—´: {paper['publish_time']}")
                print(f"   ğŸ”— é“¾æ¥: {paper['link'][:60]}...")
            
            # ä¿å­˜æ•°æ®
            success = scraper.save_to_excel(papers)
            
            if success:
                print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
            else:
                print(f"\nâš ï¸ æ•°æ®æŠ“å–æˆåŠŸï¼Œä½†ä¿å­˜å¤±è´¥")
        else:
            print(f"\nâŒ æ²¡æœ‰æŠ“å–åˆ°æ•°æ®")
            
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
