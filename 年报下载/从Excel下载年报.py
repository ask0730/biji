#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»Excelæ–‡ä»¶è¯»å–å›¾ä¹¦é¦†ä¿¡æ¯ï¼Œä¸‹è½½å»å¹´çš„å¹´æŠ¥ï¼Œå¹¶æ›´æ–°Excel
"""

import os
import re
import time
import random
import pandas as pd
import requests
from urllib.parse import urlparse, urljoin
from datetime import datetime
from bs4 import BeautifulSoup

# å°è¯•å¯¼å…¥PDFéªŒè¯åº“
try:
    import PyPDF2
    HAS_PYPDF2 = True
except ImportError:
    HAS_PYPDF2 = False
    try:
        import fitz  # PyMuPDF
        HAS_PYMUPDF = True
    except ImportError:
        HAS_PYMUPDF = False
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

def get_headers():
    """è·å–è¯·æ±‚å¤´"""
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/pdf,application/msword,application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

def setup_driver():
    """è®¾ç½®æµè§ˆå™¨é©±åŠ¨"""
    try:
        chrome_options = Options()
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        driver.implicitly_wait(10)
        return driver
    except Exception as e:
        print(f"æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
        raise

def get_last_year():
    """è·å–å»å¹´çš„å¹´ä»½"""
    return datetime.now().year - 1

def find_report_links(soup, base_url, target_year):
    """åœ¨HTMLä¸­æŸ¥æ‰¾æŒ‡å®šå¹´ä»½çš„å¹´æŠ¥é“¾æ¥"""
    report_links = []
    target_year_str = str(target_year)
    
    # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ï¼Œç­›é€‰åŒ…å«ç›®æ ‡å¹´ä»½å’Œå¹´æŠ¥ç›¸å…³çš„
    for link in soup.find_all('a', href=True):
        href = link.get('href', '')
        text = link.get_text().strip()
        
        # æ„å»ºå®Œæ•´URL
        full_url = urljoin(base_url, href)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡å¹´ä»½
        has_year = target_year_str in href or target_year_str in text
        
        # æ£€æŸ¥æ˜¯å¦ä¸å¹´æŠ¥ç›¸å…³
        is_report = any(keyword in text.lower() or keyword in href.lower() 
                       for keyword in ['å¹´æŠ¥', 'å¹´åº¦æŠ¥å‘Š', 'annual', 'report', 'å¹´åº¦'])
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶é“¾æ¥
        is_file = any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx'])
        
        if has_year and (is_report or is_file):
            report_links.append({
                'url': full_url,
                'text': text,
                'type': 'direct_link'
            })
    
    # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«å¹´ä»½çš„æ–‡æœ¬ï¼Œç„¶åæŸ¥æ‰¾é™„è¿‘çš„é“¾æ¥
    for element in soup.find_all(['div', 'li', 'td', 'tr', 'p', 'span']):
        text = element.get_text()
        if target_year_str in text and ('å¹´æŠ¥' in text or 'å¹´åº¦' in text or 'æŠ¥å‘Š' in text):
            # åœ¨è¿™ä¸ªå…ƒç´ å†…æŸ¥æ‰¾é“¾æ¥
            links = element.find_all('a', href=True)
            for link in links:
                href = link.get('href', '')
                full_url = urljoin(base_url, href)
                if full_url not in [r['url'] for r in report_links]:
                    report_links.append({
                        'url': full_url,
                        'text': link.get_text().strip() or text[:50],
                        'type': 'nearby_link'
                    })
    
    # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶é“¾æ¥ï¼Œæ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«ç›®æ ‡å¹´ä»½
    for link in soup.find_all('a', href=True):
        href = link.get('href', '').lower()
        if any(ext in href for ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx']):
            full_url = urljoin(base_url, link.get('href', ''))
            if target_year_str in full_url.lower():
                if full_url not in [r['url'] for r in report_links]:
                    report_links.append({
                        'url': full_url,
                        'text': link.get_text().strip(),
                        'type': 'file_link'
                    })
    
    # å»é‡
    seen_urls = set()
    unique_links = []
    for link in report_links:
        if link['url'] not in seen_urls:
            seen_urls.add(link['url'])
            unique_links.append(link)
    
    return unique_links

def find_report_links_selenium(driver, page_url, target_year):
    """ä½¿ç”¨Seleniumåœ¨é¡µé¢ä¸­æŸ¥æ‰¾å¹´æŠ¥é“¾æ¥"""
    try:
        driver.get(page_url)
        time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
        
        # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            pass
        
        # è·å–é¡µé¢æºç 
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        return find_report_links(soup, page_url, target_year)
        
    except Exception as e:
        print(f"  âŒ è®¿é—®é¡µé¢å¤±è´¥: {e}")
        return []

def find_report_links_requests(page_url, target_year):
    """ä½¿ç”¨requestsåœ¨é¡µé¢ä¸­æŸ¥æ‰¾å¹´æŠ¥é“¾æ¥"""
    try:
        session = requests.Session()
        session.headers.update(get_headers())
        
        response = session.get(page_url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        return find_report_links(soup, page_url, target_year)
        
    except Exception as e:
        print(f"  âŒ è¯·æ±‚å¤±è´¥: {e}")
        return []

def get_file_extension_from_url(url):
    """ä»URLè·å–æ–‡ä»¶æ‰©å±•å"""
    parsed = urlparse(url)
    path = parsed.path.lower()
    
    if path.endswith('.pdf'):
        return 'pdf'
    elif path.endswith('.doc'):
        return 'doc'
    elif path.endswith('.docx'):
        return 'docx'
    elif path.endswith('.xls'):
        return 'xls'
    elif path.endswith('.xlsx'):
        return 'xlsx'
    elif path.endswith('.ppt'):
        return 'ppt'
    elif path.endswith('.pptx'):
        return 'pptx'
    elif path.endswith('.html') or path.endswith('.htm'):
        return 'html'
    
    return 'pdf'  # é»˜è®¤

def clean_filename(filename):
    """æ¸…ç†æ–‡ä»¶å"""
    invalid_chars = r'[<>:"/\\|?*]'
    cleaned = re.sub(invalid_chars, '_', filename)
    cleaned = cleaned.strip(' .')
    if not cleaned:
        cleaned = "unnamed_file"
    return cleaned

def validate_file(file_path):
    """æ ¡éªŒæ–‡ä»¶æ˜¯å¦å¯ä»¥æ­£å¸¸æ‰“å¼€"""
    if not os.path.exists(file_path):
        return False, "æ–‡ä»¶ä¸å­˜åœ¨"
    
    file_ext = os.path.splitext(file_path)[1].lower()
    
    # æ ¡éªŒPDFæ–‡ä»¶
    if file_ext == '.pdf':
        # æ–¹æ³•1: ä½¿ç”¨PyPDF2
        if HAS_PYPDF2:
            try:
                with open(file_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    if len(pdf_reader.pages) == 0:
                        return False, "PDFæ–‡ä»¶æ²¡æœ‰é¡µé¢"
                    # å°è¯•è¯»å–ç¬¬ä¸€é¡µ
                    first_page = pdf_reader.pages[0]
                    text = first_page.extract_text()
                    return True, "PDFæ–‡ä»¶æ­£å¸¸"
            except Exception as e:
                return False, f"PDFæ–‡ä»¶æŸå: {str(e)}"
        
        # æ–¹æ³•2: ä½¿ç”¨PyMuPDF
        elif HAS_PYMUPDF:
            try:
                doc = fitz.open(file_path)
                if doc.page_count == 0:
                    doc.close()
                    return False, "PDFæ–‡ä»¶æ²¡æœ‰é¡µé¢"
                # å°è¯•è¯»å–ç¬¬ä¸€é¡µ
                first_page = doc[0]
                text = first_page.get_text()
                doc.close()
                return True, "PDFæ–‡ä»¶æ­£å¸¸"
            except Exception as e:
                return False, f"PDFæ–‡ä»¶æŸå: {str(e)}"
        
        # æ–¹æ³•3: ç®€å•æ ¡éªŒï¼ˆæ£€æŸ¥æ–‡ä»¶å¤´ï¼‰
        else:
            try:
                with open(file_path, 'rb') as f:
                    header = f.read(4)
                    if header == b'%PDF':
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦åˆç†
                        file_size = os.path.getsize(file_path)
                        if file_size < 1024:
                            return False, "PDFæ–‡ä»¶å¤§å°å¼‚å¸¸å°"
                        return True, "PDFæ–‡ä»¶æ ¼å¼æ­£ç¡®"
                    else:
                        return False, "ä¸æ˜¯æœ‰æ•ˆçš„PDFæ–‡ä»¶"
            except Exception as e:
                return False, f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"
    
    # æ ¡éªŒå…¶ä»–æ–‡ä»¶ç±»å‹ï¼ˆç®€å•æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼‰
    else:
        file_size = os.path.getsize(file_path)
        if file_size < 1024:
            return False, "æ–‡ä»¶å¤§å°å¼‚å¸¸å°"
        return True, "æ–‡ä»¶å¤§å°æ­£å¸¸"

def save_html_as_pdf(url, filename, save_dir, library_name):
    """ä½¿ç”¨Seleniumå°†HTMLé¡µé¢å®Œæ•´ä¿å­˜ä¸ºPDFï¼ˆåŒ…å«å›¾ç‰‡ç­‰èµ„æºï¼‰"""
    driver = None
    try:
        print(f"    æ­£åœ¨ä½¿ç”¨Seleniumè®¿é—®HTMLé¡µé¢...")
        driver = setup_driver()
        
        # è®¿é—®é¡µé¢
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼ˆåŒ…æ‹¬å›¾ç‰‡ï¼‰
        print(f"    ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½...")
        time.sleep(5)  # åŸºç¡€ç­‰å¾…
        
        # ç­‰å¾…æ‰€æœ‰å›¾ç‰‡åŠ è½½å®Œæˆ
        try:
            WebDriverWait(driver, 30).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )
            # é¢å¤–ç­‰å¾…å›¾ç‰‡åŠ è½½
            time.sleep(3)
            
            # æ»šåŠ¨é¡µé¢ç¡®ä¿æ‰€æœ‰å†…å®¹åŠ è½½
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
        except:
            print(f"    âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶ï¼Œç»§ç»­ä¿å­˜...")
        
        # ç¡®ä¿æ–‡ä»¶åæ˜¯PDF
        filename = clean_filename(filename)
        if not filename.lower().endswith('.pdf'):
            filename = re.sub(r'\.[^.]+$', '', filename)
            filename = f"{filename}.pdf"
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        file_path = os.path.join(save_dir, filename)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
            except:
                pass
        
        # ä½¿ç”¨Chromeçš„æ‰“å°åŠŸèƒ½ä¿å­˜ä¸ºPDF
        print(f"    æ­£åœ¨å°†é¡µé¢ä¿å­˜ä¸ºPDF...")
        
        # è®¾ç½®æ‰“å°é€‰é¡¹
        print_options = {
            'printBackground': True,  # åŒ…å«èƒŒæ™¯å›¾ç‰‡å’Œé¢œè‰²
            'paperWidth': 8.27,  # A4å®½åº¦ï¼ˆè‹±å¯¸ï¼‰
            'paperHeight': 11.69,  # A4é«˜åº¦ï¼ˆè‹±å¯¸ï¼‰
            'marginTop': 0.4,
            'marginBottom': 0.4,
            'marginLeft': 0.4,
            'marginRight': 0.4,
        }
        
        # æ‰§è¡Œæ‰“å°å‘½ä»¤
        result = driver.execute_cdp_cmd('Page.printToPDF', print_options)
        
        # ä¿å­˜PDF
        import base64
        pdf_data = base64.b64decode(result['data'])
        
        with open(file_path, 'wb') as f:
            f.write(pdf_data)
        
        # éªŒè¯æ–‡ä»¶
        if not os.path.exists(file_path):
            print(f"    âŒ PDFä¿å­˜å¤±è´¥")
            return False
        
        file_size = os.path.getsize(file_path)
        
        if file_size < 1024:
            print(f"    âš ï¸ æ–‡ä»¶å¤§å°å¼‚å¸¸å° ({file_size} å­—èŠ‚)ï¼Œå¯èƒ½æ˜¯é”™è¯¯é¡µé¢")
            os.remove(file_path)
            return False
        
        print(f"    âœ… PDFä¿å­˜æˆåŠŸ: {filename}")
        print(f"       æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚ ({file_size/1024/1024:.2f} MB)")
        
        return True, file_path
        
    except Exception as e:
        print(f"    âŒ ä¿å­˜HTMLä¸ºPDFå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

def download_file(url, filename, save_dir, library_name, check_html_pdf=True):
    """ä¸‹è½½æ–‡ä»¶
    Args:
        url: æ–‡ä»¶URL
        filename: æ–‡ä»¶å
        save_dir: ä¿å­˜ç›®å½•
        library_name: å›¾ä¹¦é¦†åç§°
        check_html_pdf: æ˜¯å¦æ£€æŸ¥HTMLé¡µé¢ä¸­çš„PDFé“¾æ¥ï¼ˆé¿å…é€’å½’æ—¶é‡å¤æ£€æŸ¥ï¼‰
    """
    try:
        session = requests.Session()
        session.headers.update(get_headers())
        
        print(f"    æ­£åœ¨ä¸‹è½½: {url[:80]}...")
        
        # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯HTMLé¡µé¢
        is_html_page = False
        try:
            head_response = session.head(url, timeout=30, allow_redirects=True)
            content_type = head_response.headers.get('Content-Type', '').lower()
            
            if 'text/html' in content_type:
                is_html_page = True
        except:
            # HEADè¯·æ±‚å¤±è´¥ï¼Œæ£€æŸ¥URLæ‰©å±•å
            if url.lower().endswith(('.html', '.htm')):
                is_html_page = True
        
        # å¦‚æœæ˜¯HTMLé¡µé¢ï¼Œå…ˆå°è¯•æŸ¥æ‰¾å…¶ä¸­çš„PDFä¸‹è½½é“¾æ¥
        if is_html_page and check_html_pdf:
            print(f"    âš ï¸ URLæŒ‡å‘HTMLé¡µé¢ï¼Œå…ˆæŸ¥æ‰¾å…¶ä¸­çš„PDFä¸‹è½½é“¾æ¥...")
            
            # ä½¿ç”¨Seleniumè®¿é—®é¡µé¢ï¼ŒæŸ¥æ‰¾PDFé“¾æ¥
            driver = None
            try:
                driver = setup_driver()
                driver.get(url)
                time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                try:
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                except TimeoutException:
                    pass
                
                # è·å–é¡µé¢æºç 
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
                pdf_links = []
                for link in soup.find_all('a', href=True):
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    full_url = urljoin(url, href)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯PDFé“¾æ¥
                    if '.pdf' in href.lower() or '.pdf' in full_url.lower():
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½æˆ–å¹´æŠ¥ç›¸å…³å…³é”®è¯
                        if str(get_last_year()) in text or 'å¹´æŠ¥' in text or 'å¹´åº¦' in text or 'æŠ¥å‘Š' in text:
                            pdf_links.append({
                                'url': full_url,
                                'text': text
                            })
                
                # å¦‚æœæ‰¾åˆ°PDFé“¾æ¥ï¼Œå°è¯•ä¸‹è½½ç¬¬ä¸€ä¸ª
                if pdf_links:
                    print(f"    âœ… åœ¨HTMLé¡µé¢ä¸­æ‰¾åˆ° {len(pdf_links)} ä¸ªPDFä¸‹è½½é“¾æ¥")
                    pdf_url = pdf_links[0]['url']
                    print(f"    å°è¯•ä¸‹è½½PDF: {pdf_url[:80]}...")
                    
                    # é€’å½’è°ƒç”¨download_fileä¸‹è½½PDFï¼ˆè®¾ç½®check_html_pdf=Falseé¿å…é‡å¤æ£€æŸ¥ï¼‰
                    result, file_path = download_file(pdf_url, filename, save_dir, library_name, check_html_pdf=False)
                    if result and file_path:
                        return result, file_path
                    else:
                        print(f"    âŒ PDFé“¾æ¥ä¸‹è½½å¤±è´¥")
                        return False, None
                else:
                    print(f"    âŒ HTMLé¡µé¢ä¸­æœªæ‰¾åˆ°PDFé“¾æ¥")
                    return False, None
                
            except Exception as e:
                print(f"    âŒ æŸ¥æ‰¾PDFé“¾æ¥å¤±è´¥: {e}")
                return False, None
            finally:
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
        
        # ä¸‹è½½æ–‡ä»¶
        print(f"    æ­£åœ¨è¿æ¥æœåŠ¡å™¨...")
        response = session.get(url, stream=True, timeout=30, allow_redirects=True)
        response.raise_for_status()
        
        # è·å–æ–‡ä»¶å¤§å°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        total_size = int(response.headers.get('Content-Length', 0))
        if total_size > 0:
            total_size_mb = total_size / 1024 / 1024
            print(f"    æ–‡ä»¶å¤§å°: {total_size_mb:.2f} MB")
        
        # ç¡®å®šæ–‡ä»¶æ‰©å±•å
        file_ext = get_file_extension_from_url(url)
        content_type = response.headers.get('Content-Type', '').lower()
        
        # å¦‚æœä¸‹è½½çš„æ˜¯HTMLï¼Œä½¿ç”¨Seleniumä¿å­˜ä¸ºPDF
        if 'text/html' in content_type or file_ext == 'html':
            print(f"    âš ï¸ ä¸‹è½½çš„æ–‡ä»¶æ˜¯HTMLï¼Œå°†è½¬æ¢ä¸ºPDF...")
            # å…ˆä¿å­˜HTMLåˆ°ä¸´æ—¶æ–‡ä»¶
            temp_html_path = os.path.join(save_dir, f"temp_{int(time.time())}.html")
            with open(temp_html_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # ä½¿ç”¨Seleniumè½¬æ¢ä¸ºPDF
            result, file_path = save_html_as_pdf(f"file:///{temp_html_path.replace(os.sep, '/')}", filename, save_dir, library_name)
            
            # åˆ é™¤ä¸´æ—¶HTMLæ–‡ä»¶
            try:
                if os.path.exists(temp_html_path):
                    os.remove(temp_html_path)
            except:
                pass
            
            return result, file_path
        
        if 'pdf' in content_type:
            file_ext = 'pdf'
        elif 'msword' in content_type or 'wordprocessingml' in content_type:
            file_ext = 'docx' if 'openxml' in content_type else 'doc'
        elif 'spreadsheetml' in content_type:
            file_ext = 'xlsx' if 'openxml' in content_type else 'xls'
        elif 'presentationml' in content_type:
            file_ext = 'pptx' if 'openxml' in content_type else 'ppt'
        
        # ç¡®ä¿æ–‡ä»¶åæœ‰æ­£ç¡®çš„æ‰©å±•å
        if not filename.lower().endswith(f'.{file_ext}'):
            filename = re.sub(r'\.[^.]+$', '', filename)
            filename = f"{filename}.{file_ext}"
        
        filename = clean_filename(filename)
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        file_path = os.path.join(save_dir, filename)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
            except:
                pass
        
        # ä¿å­˜æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦
        print(f"    å¼€å§‹ä¸‹è½½...")
        downloaded_size = 0
        start_time = time.time()
        last_print_time = start_time
        
        with open(file_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    
                    # æ¯0.5ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦æ˜¾ç¤º
                    current_time = time.time()
                    if current_time - last_print_time >= 0.5:
                        if total_size > 0:
                            percent = (downloaded_size / total_size) * 100
                            speed = downloaded_size / (current_time - start_time) / 1024  # KB/s
                            print(f"    è¿›åº¦: {percent:.1f}% ({downloaded_size/1024/1024:.2f} MB / {total_size/1024/1024:.2f} MB) - é€Ÿåº¦: {speed:.1f} KB/s", end='\r')
                        else:
                            speed = downloaded_size / (current_time - start_time) / 1024  # KB/s
                            print(f"    å·²ä¸‹è½½: {downloaded_size/1024/1024:.2f} MB - é€Ÿåº¦: {speed:.1f} KB/s", end='\r')
                        last_print_time = current_time
        
        # ä¸‹è½½å®Œæˆï¼Œæ¢è¡Œ
        print()  # æ¢è¡Œ
        
        # éªŒè¯æ–‡ä»¶
        if not os.path.exists(file_path):
            print(f"    âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥")
            return False
        
        file_size = os.path.getsize(file_path)
        
        if file_size < 1024:
            print(f"    âš ï¸ æ–‡ä»¶å¤§å°å¼‚å¸¸å° ({file_size} å­—èŠ‚)ï¼Œå¯èƒ½æ˜¯é”™è¯¯é¡µé¢")
            os.remove(file_path)
            return False
        
        print(f"    âœ… ä¸‹è½½æˆåŠŸ: {filename}")
        print(f"       æ–‡ä»¶ç±»å‹: {file_ext.upper()}")
        print(f"       æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚ ({file_size/1024/1024:.2f} MB)")
        
        return True, file_path
        
    except Exception as e:
        print(f"    âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False, None

def process_library_row(row, library_name_col, url_col, save_dir, target_year):
    """å¤„ç†Excelä¸­çš„ä¸€è¡Œæ•°æ®"""
    library_name = str(row[library_name_col]).strip()
    page_url = str(row[url_col]).strip()
    
    # è·³è¿‡ç©ºå€¼ï¼ˆåªæ£€æŸ¥å›¾ä¹¦é¦†åç§°ï¼‰
    if pd.isna(row[library_name_col]) or library_name == 'nan' or not library_name:
        return False, None, None
    
    # åœ°å€ä¸ºç©ºä¹Ÿè¦å¤„ç†ï¼Œä¸è·³è¿‡
    if pd.isna(row[url_col]) or page_url == 'nan' or not page_url:
        print(f"\nâš ï¸ {library_name}: å¹´æŠ¥åœ°å€ä¸ºç©ºï¼Œå°†æ ‡è®°ä¸ºå¤±è´¥")
        return False, None, None
    
    print(f"\n{'='*60}")
    print(f"å¤„ç†: {library_name}")
    print(f"å¹´æŠ¥åœ°å€: {page_url}")
    print(f"{'='*60}")
    
    # å…ˆå°è¯•ä½¿ç”¨requests
    report_links = find_report_links_requests(page_url, target_year)
    
    # å¦‚æœrequestså¤±è´¥æˆ–æ²¡æ‰¾åˆ°é“¾æ¥ï¼Œä½¿ç”¨Selenium
    if not report_links:
        print("  ä½¿ç”¨Seleniumè®¿é—®é¡µé¢...")
        driver = setup_driver()
        try:
            report_links = find_report_links_selenium(driver, page_url, target_year)
        finally:
            driver.quit()
    
    if not report_links:
        print(f"  âš ï¸ æœªæ‰¾åˆ°{target_year}å¹´å¹´æŠ¥é“¾æ¥")
        return False, None, None
    
    print(f"  âœ… æ‰¾åˆ° {len(report_links)} ä¸ªå¯èƒ½çš„å¹´æŠ¥é“¾æ¥")
    
    # ä¸‹è½½æ‰¾åˆ°çš„é“¾æ¥
    success = False
    downloaded_url = None
    downloaded_file_path = None
    for i, link_info in enumerate(report_links, 1):
        url = link_info['url']
        link_text = link_info['text']
        
        print(f"\n  å°è¯•ä¸‹è½½é“¾æ¥ {i}/{len(report_links)}:")
        print(f"    æ–‡æœ¬: {link_text[:50]}...")
        
        # ç”Ÿæˆæ–‡ä»¶å
        filename = f"{library_name}{target_year}å¹´å¹´æŠ¥"
        
        result, file_path = download_file(url, filename, save_dir, library_name)
        if result and file_path:
            success = True
            downloaded_url = url  # è®°å½•æˆåŠŸä¸‹è½½çš„URL
            downloaded_file_path = file_path  # è®°å½•æ–‡ä»¶è·¯å¾„
            break  # æˆåŠŸä¸‹è½½ä¸€ä¸ªå°±å¤Ÿäº†
        else:
            print(f"    âš ï¸ è¯¥é“¾æ¥ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
    
    return success, downloaded_url, downloaded_file_path

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, "config.txt")
    config = {}
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    if '=' in line:
                        key, value = line.split('=', 1)
                        config[key.strip()] = value.strip()
        except Exception as e:
            print(f"âš ï¸ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    return config

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ä»Excelä¸‹è½½å›¾ä¹¦é¦†å¹´æŠ¥å·¥å…·")
    print("=" * 60)
    
    # è‡ªåŠ¨è·å–Excelæ–‡ä»¶è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„Excelæ–‡ä»¶
    excel_files = [f for f in os.listdir(script_dir) if f.endswith(('.xlsx', '.xls'))]
    if excel_files:
        # è‡ªåŠ¨ä½¿ç”¨ç¬¬ä¸€ä¸ªExcelæ–‡ä»¶
        excel_file = excel_files[0]
        excel_file = os.path.join(script_dir, excel_file)
        print(f"\nğŸ“„ è‡ªåŠ¨ä½¿ç”¨Excelæ–‡ä»¶: {os.path.basename(excel_file)}")
    else:
        print("âŒ å½“å‰ç›®å½•ä¸‹æœªæ‰¾åˆ°Excelæ–‡ä»¶")
        return
    
    if not os.path.exists(excel_file):
        print(f"âŒ Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_file}")
        return
    
    print(f"\nğŸ“„ Excelæ–‡ä»¶: {excel_file}")
    
    # è¯»å–Excelæ–‡ä»¶
    try:
        df = pd.read_excel(excel_file, engine='openpyxl')
        print(f"âœ… æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…± {len(df)} è¡Œæ•°æ®")
        print(f"   åˆ—å: {list(df.columns)}")
    except Exception as e:
        print(f"âŒ è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # ç¡®å®šåˆ—å
    print(f"\nè¯·ç¡®è®¤åˆ—å:")
    library_name_col = None
    url_col = None
    
    # è‡ªåŠ¨è¯†åˆ«åˆ—å
    for col in df.columns:
        col_lower = str(col).lower()
        if 'å›¾ä¹¦é¦†' in str(col) or 'åç§°' in col_lower or 'name' in col_lower:
            library_name_col = col
        elif 'åœ°å€' in str(col) or 'url' in col_lower or 'é“¾æ¥' in str(col) or 'å¹´æŠ¥' in str(col):
            url_col = col
    
    if not library_name_col:
        print(f"\nå¯ç”¨åˆ—: {list(df.columns)}")
        library_name_col = input("è¯·è¾“å…¥å›¾ä¹¦é¦†åç§°åˆ—å: ").strip()
    
    if not url_col:
        print(f"\nå¯ç”¨åˆ—: {list(df.columns)}")
        url_col = input("è¯·è¾“å…¥å¹´æŠ¥åœ°å€åˆ—å: ").strip()
    
    if library_name_col not in df.columns or url_col not in df.columns:
        print(f"âŒ åˆ—åä¸æ­£ç¡®")
        return
    
    print(f"\nâœ… ä½¿ç”¨åˆ—:")
    print(f"   å›¾ä¹¦é¦†åç§°: {library_name_col}")
    print(f"   å¹´æŠ¥åœ°å€: {url_col}")
    
    # æ™ºèƒ½è¯†åˆ«çŠ¶æ€åˆ—ï¼ˆä¼˜å…ˆæŸ¥æ‰¾åŒ…å«"æ˜¯å¦"ã€"ä¸‹è½½"ã€"çŠ¶æ€"çš„åˆ—ï¼‰
    status_col = None
    status_keywords = ['æ˜¯å¦', 'ä¸‹è½½', 'çŠ¶æ€', 'æ˜¯å¦ä¸‹è½½', 'ä¸‹è½½çŠ¶æ€']
    
    # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„åˆ—
    for col in df.columns:
        col_str = str(col)
        if col_str != library_name_col and col_str != url_col:
            for keyword in status_keywords:
                if keyword in col_str:
                    status_col = col
                    print(f"âœ… è‡ªåŠ¨è¯†åˆ«çŠ¶æ€åˆ—: {status_col}")
                    break
            if status_col:
                break
    
    # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æœ€åä¸€åˆ—
    if not status_col:
        last_col_name = df.columns[-1]
        if last_col_name != library_name_col and last_col_name != url_col:
            status_col = last_col_name
            print(f"âœ… ä½¿ç”¨æœ€åä¸€åˆ—ä½œä¸ºçŠ¶æ€åˆ—: {status_col}")
        else:
            # å¦‚æœæœ€åä¸€åˆ—æ˜¯æ•°æ®åˆ—ï¼Œæ·»åŠ æ–°åˆ—
            df['ä¸‹è½½çŠ¶æ€'] = ''
            status_col = 'ä¸‹è½½çŠ¶æ€'
            print(f"âœ… åˆ›å»ºæ–°çŠ¶æ€åˆ—: {status_col}")
    
    print(f"   çŠ¶æ€åˆ—: {status_col}")
    
    # ç¡®ä¿çŠ¶æ€åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆé¿å…ç±»å‹è½¬æ¢é”™è¯¯ï¼‰
    df[status_col] = df[status_col].astype(str)
    # å°†'nan'å­—ç¬¦ä¸²æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
    df[status_col] = df[status_col].replace('nan', '')
    
    # æ·»åŠ å¹´æŠ¥ä¸‹è½½åœ°å€åˆ—
    report_url_col = 'å¹´æŠ¥ä¸‹è½½åœ°å€'
    if report_url_col not in df.columns:
        df[report_url_col] = ''
        print(f"âœ… å·²æ·»åŠ æ–°åˆ—: {report_url_col}")
    else:
        print(f"âœ… ä½¿ç”¨å·²æœ‰åˆ—: {report_url_col}")
    
    # ç¡®ä¿å¹´æŠ¥ä¸‹è½½åœ°å€åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
    df[report_url_col] = df[report_url_col].astype(str)
    df[report_url_col] = df[report_url_col].replace('nan', '')
    
    # åŠ è½½é…ç½®
    config = load_config()
    output_folder = config.get("output_folder", "").strip()
    if output_folder:
        save_dir = output_folder
    else:
        save_dir = os.path.join(script_dir, "ä¸‹è½½çš„å¹´æŠ¥")
    
    print(f"\nğŸ“ ä¿å­˜ç›®å½•: {save_dir}")
    os.makedirs(save_dir, exist_ok=True)
    
    target_year = get_last_year()
    print(f"\nğŸ“… ç›®æ ‡å¹´ä»½: {target_year}å¹´ï¼ˆå»å¹´ï¼‰")
    print("=" * 60)
    
    # å¤„ç†æ¯ä¸€è¡Œ
    success_count = 0
    fail_count = 0
    already_done_count = 0
    
    for index, row in df.iterrows():
        library_name = str(row[library_name_col]).strip()
        
        # æ£€æŸ¥ä¸‹è½½çŠ¶æ€ï¼Œå¦‚æœçŠ¶æ€ä¸º"æ˜¯"åˆ™è·³è¿‡ï¼Œå¦åˆ™æŠ“å–
        current_status = str(row[status_col]).strip()
        # å¤„ç†å¯èƒ½çš„ç‰¹æ®Šæƒ…å†µï¼šå»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼Œç»Ÿä¸€å¤§å°å†™
        current_status_clean = current_status.replace(' ', '').replace('\t', '').replace('\n', '')
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºè¯»å–åˆ°çš„çŠ¶æ€å€¼
        if index < 3:  # åªæ˜¾ç¤ºå‰3è¡Œçš„è°ƒè¯•ä¿¡æ¯
            print(f"\n[è°ƒè¯•] ç¬¬{index+1}è¡Œ - å›¾ä¹¦é¦†: {library_name}, çŠ¶æ€åˆ—: '{status_col}', çŠ¶æ€å€¼: '{current_status}' (æ¸…ç†å: '{current_status_clean}')")
        
        # å¦‚æœçŠ¶æ€ä¸º"æ˜¯"ï¼ˆè€ƒè™‘å„ç§å¯èƒ½çš„æ ¼å¼ï¼‰ï¼Œè·³è¿‡
        if current_status_clean == 'æ˜¯' or current_status_clean.lower() == 'yes' or current_status_clean == '1':
            print(f"\nâ­ï¸  è·³è¿‡: {library_name}ï¼ˆçŠ¶æ€ä¸º'æ˜¯'ï¼Œå·²ä¸‹è½½ï¼‰")
            already_done_count += 1
            continue
        
        # çŠ¶æ€ä¸º"å¦"ã€ç©ºå€¼æˆ–å…¶ä»–ï¼Œéƒ½è¿›è¡ŒæŠ“å–
        print(f"\nğŸ“¥ å¼€å§‹å¤„ç†: {library_name}ï¼ˆçŠ¶æ€: '{current_status if current_status else 'ç©º'}ï¼‰")
        
        try:
            result, downloaded_url, downloaded_file_path = process_library_row(row, library_name_col, url_col, save_dir, target_year)
            if result and downloaded_file_path:
                # æ ¡éªŒæ–‡ä»¶æ˜¯å¦å¯ä»¥æ­£å¸¸æ‰“å¼€
                print(f"  ğŸ” æ­£åœ¨æ ¡éªŒæ–‡ä»¶...")
                is_valid, validation_msg = validate_file(downloaded_file_path)
                
                if is_valid:
                    print(f"  âœ… æ–‡ä»¶æ ¡éªŒé€šè¿‡: {validation_msg}")
                    df.at[index, status_col] = 'æ˜¯'
                    if downloaded_url:
                        df.at[index, report_url_col] = downloaded_url
                    success_count += 1
                else:
                    print(f"  âŒ æ–‡ä»¶æ ¡éªŒå¤±è´¥: {validation_msg}")
                    # åˆ é™¤æŸåçš„æ–‡ä»¶
                    try:
                        if os.path.exists(downloaded_file_path):
                            os.remove(downloaded_file_path)
                            print(f"  ğŸ—‘ï¸  å·²åˆ é™¤æŸåçš„æ–‡ä»¶: {os.path.basename(downloaded_file_path)}")
                    except Exception as e:
                        print(f"  âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
                    # æ›´æ–°çŠ¶æ€ä¸º"å¦"
                    df.at[index, status_col] = 'å¦'
                    df.at[index, report_url_col] = ''
                    print(f"  ğŸ“ å·²å°†çŠ¶æ€æ›´æ–°ä¸º'å¦'")
                    fail_count += 1
                    
                    # ç«‹å³ä¿å­˜Excelï¼Œç¡®ä¿çŠ¶æ€æ›´æ–°è¢«ä¿å­˜
                    try:
                        df.to_excel(excel_file, index=False, engine='openpyxl')
                        print(f"  ğŸ’¾ å·²ä¿å­˜çŠ¶æ€æ›´æ–°åˆ°Excel")
                    except Exception as e:
                        print(f"  âš ï¸ ä¿å­˜Excelå¤±è´¥: {e}")
            elif result:
                # ä¸‹è½½æˆåŠŸä½†æ²¡æœ‰æ–‡ä»¶è·¯å¾„ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                print(f"  âš ï¸ ä¸‹è½½æˆåŠŸä½†æœªè¿”å›æ–‡ä»¶è·¯å¾„")
                df.at[index, status_col] = 'å¦'
                df.at[index, report_url_col] = ''
                fail_count += 1
            else:
                # ä¸‹è½½å¤±è´¥ï¼Œæ›´æ–°çŠ¶æ€ä¸º"å¦"
                page_url = str(row[url_col]).strip()
                df.at[index, status_col] = 'å¦'
                df.at[index, report_url_col] = ''  # å¤±è´¥æ—¶æ¸…ç©ºåœ°å€
                if pd.isna(row[url_col]) or page_url == 'nan' or not page_url:
                    print(f"  âš ï¸ å¹´æŠ¥åœ°å€ä¸ºç©ºï¼Œå·²æ ‡è®°ä¸º'å¦'")
                else:
                    print(f"  âŒ ä¸‹è½½å¤±è´¥ï¼Œå·²æ ‡è®°ä¸º'å¦'")
                fail_count += 1
                
                # ç«‹å³ä¿å­˜Excelï¼Œç¡®ä¿çŠ¶æ€æ›´æ–°è¢«ä¿å­˜
                try:
                    df.to_excel(excel_file, index=False, engine='openpyxl')
                    print(f"  ğŸ’¾ å·²ä¿å­˜çŠ¶æ€æ›´æ–°åˆ°Excel")
                except Exception as e:
                    print(f"  âš ï¸ ä¿å­˜Excelå¤±è´¥: {e}")
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            df.at[index, status_col] = 'å¦'
            df.at[index, report_url_col] = ''  # å¤±è´¥æ—¶æ¸…ç©ºåœ°å€
            fail_count += 1
        
        # æ¯ä¸ªå›¾ä¹¦é¦†ä¹‹é—´ç¨ä½œå»¶è¿Ÿ
        time.sleep(2)
        
        # æ¯å¤„ç†5ä¸ªä¿å­˜ä¸€æ¬¡ï¼ˆé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰
        if (index + 1) % 5 == 0:
            try:
                df.to_excel(excel_file, index=False, engine='openpyxl')
                print(f"\nğŸ’¾ å·²ä¿å­˜è¿›åº¦åˆ°Excelæ–‡ä»¶ï¼ˆå·²å¤„ç† {index + 1}/{len(df)} è¡Œï¼‰")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜Excelå¤±è´¥: {e}")
    
    # æœ€ç»ˆä¿å­˜
    try:
        df.to_excel(excel_file, index=False, engine='openpyxl')
        print(f"\nâœ… Excelæ–‡ä»¶å·²æ›´æ–°å¹¶ä¿å­˜")
    except Exception as e:
        print(f"âŒ ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ä¸‹è½½å®Œæˆ")
    print("=" * 60)
    print(f"âœ… æˆåŠŸ: {success_count} ä¸ª")
    print(f"âŒ å¤±è´¥: {fail_count} ä¸ªï¼ˆåŒ…æ‹¬åœ°å€ä¸ºç©ºçš„æƒ…å†µï¼‰")
    print(f"âœ“ å·²å­˜åœ¨: {already_done_count} ä¸ªï¼ˆçŠ¶æ€ä¸º'æ˜¯'ï¼Œå·²è·³è¿‡ï¼‰")
    print(f"ğŸ“Š æ€»è®¡: {len(df)} ä¸ª")
    print(f"ğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {save_dir}")
    print(f"ğŸ“„ Excelæ–‡ä»¶å·²æ›´æ–°: {excel_file}")
    print("=" * 60)

if __name__ == "__main__":
    main()

